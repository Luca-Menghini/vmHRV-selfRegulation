---
title: "Inter- and intraindividual relationships between vagally-mediated heart rate variability and self-regulatory processes: An ecological momentary assessment"
subtitle: "Supplementary material 2: Data pre-processing"
author: "Luca Menghini, MS$^1$, Giulia Fuochi, PhD$^2$, Michela Sarlo, PhD$^3$"
date:  "`r Sys.Date()`"
bibliography: [packagesProc.bib]
nocite: '@*'
output: 
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float: true
    css: styles.css
---

<br>

$^1$Department of General Psychology, University of Padova, Italy

$^2$Department of Philosophy, Sociology, Pedagogy and Applied Psychology, University of Padova, Italy

$^3$Department of Communication Sciences, Humanities and International Studies, University of Urbino Carlo Bo, Italy

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

# Aims and content

The present document includes the data pre-processing steps used to read the different types of data collected over three days from a sample of 105 healthy adults:

- `PrelQS` = demographic & retrospective self-report data collected with the preliminary questionnaire ([Google Forms](https://www.google.com/intl/it/forms/about/))

- `ESM` = experience sampling measures collected with the [Sensus Mobile app](https://predictive-technology-laboratory.github.io/sensus/) ([Xiong et al., 2016](#ref))

- `HRV` = 2-min segments of blood volume pulse data recorded with the E4 wristband (Empatica, Milan)

- `GNG` = Go/No-Go behavioral data recorded with E-Prime 2.0.10 (Psychology Software Tools, Inc., Sharpsburg, PA) 

<br>

For each data type, the following pre-processing steps are implemented to generate the `wide` and `long` datasets to be used for subsequent data analysis (see analytical reports [1. Psychometrics and descriptives](https://luca-menghini/vmHRV-selfRegulation/) and [2. Regression models](https://luca-menghini/vmHRV-selfRegulation/)):

1. Raw data files **reading & merging** 

2. Raw data **recoding & processing**

3. Data **filtering** based on inclusion criteria and data quality

<br>

Here, we remove all objects from the R global environment, and we set the system time zone for better temporal synchronization across data files.
```{r warning=FALSE}
# removing all objets from the workspace
rm(list=ls())

# setting system time zone to GMT (for consistent temporal synchronization)
Sys.setenv(tz="GMT")
```

<br>

The following R packages are used in this document (see [References section](#ref)):
```{r }
# required packages
packages <- c("ggplot2","gridExtra","jsonlite","plyr","lubridate","scales","knitr","reshape2","careless")

# generate packages references
knitr::write_bib(c(.packages(), packages),"packagesProc.bib")

# # run to install missing packages
# xfun::pkg_attach2(packages, message = FALSE); rm(list=ls())
```

<br>
<br>

# 1. PrelQS data

Here, the retrospective self-report data collected with the **preliminary questionnaire** are read and saved in the `PrelQS` dataset, recoded, and filtered based on the inclusion criteria.
```{r warning=FALSE,message=FALSE}
library(ggplot2);library(gridExtra) # loading required packages
```

<br>

## 1.1. Data reading

First, we read the `Preliminary_qs.csv` data file exported from [Google Forms](https://www.google.com/intl/it/forms/about/).
```{r }
PrelQS <- read.csv("qs preliminare/Preliminary_qs.csv")
```

<br>

## 1.2. Data recoding

Then, we recode the `PrelQS` variables to be used for the analyses.

### 1.2.1. ID recoding

As a first step, we **recode participants' identification codes** `ID` (currently corresponding to their e-mail addresses) using the "HRVXXX" format (e.g., from '[john.smith\@gmail.com](mailto:john.smith@gmail.com){.email}' to 'HRV001'). This is done with the `prel.qs_IDrecode()` function. Since the participants' e-mail addresses are confidential, both the function and the original dataset are not showed. 
```{r }
# renaming the first two variables
colnames(PrelQS)[1:2] <- c("timestamp","ID") 

# loading and using the function to recode the ID variable
source("prel.qs_IDrecode.R") 
(PrelQS <- prel.qs_IDrecode(PrelQS))[1:3,] # showing first three rows
```

<br>

### 1.2.2. Other variables

Second, we rename the other variables, remove those not considered for the present study, and recode the considered variables to be used in the analyses.
```{r }
# renaming all variables
colnames(PrelQS) <- c("time","ID", # submission timestamp & participants ID
                      "consent", # consent to participate (all 'yes')
                      "age","gender","weight","height","itaMT", # demographics
                      
                      # inclusion criteria
                      "drugs","drugs.which","cvDysf","cvDysf.which",
                      "otherDysf","otherDysf.which","phone","phone.which",
                      
                      # retrospective scales (* = not considered in this study)
                      paste("DASS21",1:21,sep=""), # Depression, Anxiety, and Stress Scale*
                      paste("PANAS",1:20,sep=""), # Positive and Negative Affective Schedule*
                      paste("DERS",1:36,sep=""), # Difficulties in Emotion Regulation Scale*
                      paste("BIS11",c(1:15,17:20,22,23,25:30),sep=""), # Barratt Impulsiveness Scale-11 (3 items out)
                      paste("PSI",1:17,sep="")) # Physical Symptoms Inventory*

# keeping only considered variables (demographics and inclusion criteria)
PrelQS <- PrelQS[,c("ID","time",colnames(PrelQS)[which(colnames(PrelQS)=="age"):which(colnames(PrelQS)=="height")],
                    colnames(PrelQS)[which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")])]

# recoding variables
PrelQS[,c("ID","gender")] <- lapply(PrelQS[,c("ID","gender")],as.factor) # ID and gender as factor
PrelQS$time <- as.POSIXct(PrelQS$time,format="%Y/%m/%d %I:%M:%S %p") # time as POSIXct
PrelQS[PrelQS$height<3,"height"] <- PrelQS[PrelQS$height<3,"height"]*100 # correcting heights reported in meters
PrelQS <- cbind(PrelQS[,1:4],BMI=PrelQS$weight/(PrelQS$height/100)^2, # computing BMI, removing weight & height
                PrelQS[,7:ncol(PrelQS)])
for(i in which(colnames(PrelQS)%in%c("drugs","cvDysf","otherDysf","phone"))){ PrelQS[,i] <- gsub("Sì","Yes",PrelQS[,i]) }
PrelQS[,which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")] <- # inclusion criteria as factors
  lapply(PrelQS[,which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")],as.factor)

# sorting dataset by ID and time
PrelQS <- PrelQS[order(PrelQS$ID,PrelQS$time),]
```

<br>

### 1.2.3. Data structure

Here, we display the structure of the `PrelQS` processed dataset.
```{r }
str(PrelQS)
```

<br>

## 1.3. Cleaning & filtering

Here, we clean and filter the data based on the inclusion criteria and by focusing on those participants that were actually invited to participate to the daily protocol (N = 105).

First, we inspect the original number of cases in the `PrelQS` dataset. The questionnaire was completed by a total of 164 respondents, of which a subsample was invited to participate in the daily protocol. 
```{r }
cat("Original No. of responses to the PrelQS =",nrow(PrelQS))
```

<br>

### 1.3.1. Double responses

First, we inspect and remove cases of **double responses** (N = 3 couples of responses with the same `ID` value). In these cases, only the first response (i.e., the one with the earilest timestamp) is included.
```{r }
# detecting double responses
cat(nrow(PrelQS[duplicated(PrelQS$ID),]),"cases of double responses (same ID)")

# removing double responses
memory <- PrelQS
PrelQS <- PrelQS[!duplicated(PrelQS$ID),]
cat("Removed",nrow(memory)-nrow(PrelQS),"double responses")
```

<br>

### 1.3.2. Inclusion criteria {.tabset .tabset-fade .tabset-pills}

Second, we take a look at the variables concerning the **inclusion criteria** of the study:

1. Not suffering from **dysfunctions** (e.g., anxiety disorder) or taking **medications** affecting the *nervous system* (e.g., antidepressants)

2. Not suffering from **dysfunctions** (e.g., hypertension) or taking **medications** affecting the *cardiovascular system* (e.g., beta blockers)

3. Owning an Android or iOS **smartphone**

<br>

In section 1.2.1., we recoded participants identification codes and we marked the cases of participants not meeting the inclusion criteria, as well as other cases of participants that were not invited to take part in the EMA protocol for other reasons, using the "***OUT***" label in the `ID` variable. From a total of 161 responses to the preliminary questionnaire, we can see that 105 participants (65%) were actually invited to participate in the daily protocol.
```{r }
cat("Total No. of responses to the PrelQS =",nrow(PrelQS),"\n -",
    nrow(PrelQS[substr(PrelQS$ID,1,3)!="OUT",]),"invited\n -",nrow(PrelQS[substr(PrelQS$ID,1,3)=="OUT",]),"not invited")
```

<br>

Here, we better evaluate the reasons for the exclusion of the remaining 56 participants.

#### SUMMARY

- **9** participants were not invited because they reported suffering from **cardiovascular** (i.e., premature heart beat, problems with the mitral valve) or **other dysfunction** (i.e., thalassemia, anxiety, Crohn’s disease) and/or taking **antidepressants**

- **1** participant was not invited because she did not own a smartphone

- Further **46** participants were not invited to join the EMA protocol for other reasons (e.g., calendar incompatibilities, end of the data collection)

<br>

Here, we compare the demographic and retrospective variables between included and excluded participants. Most participants that were not invited to take part in the daily protocol were **female** (probably due to the higher No. of female compared to undergraduates participating to psychological studies, and our goal of having a balanced sample), with no marked differences in terms of `gender`, `age`, `BMI`
```{r fig.width=10,fig.height=2}
# preparing data
PrelQS$excl <- "IN" # creating excl variable (factor)
PrelQS[substr(PrelQS$ID,1,3)=="OUT","excl"] <- "OUT"
PrelQS$excl <- as.factor(PrelQS$excl)

# plotting
grid.arrange(ggplot(PrelQS,aes(x=excl,fill=gender)) + geom_bar(),
             ggplot(PrelQS,aes(x=excl,y=age,fill=excl)) + geom_violin(),
             ggplot(PrelQS,aes(x=excl,y=BMI,fill=excl)) + geom_violin(),nrow=1) 

# removing excl
PrelQS$excl <- NULL
```

<br>

#### DYSFUNCTIONS (7)

Here, we show the No. of participants not invited to take part in the EMA protocol due to **dysfunctions** affecting either the *cardiovascular* or the *nervous* system. We can see that 4 females were excluded since they reported suffering from a **cardiovascular dysfunction** including premature heart beat (`027`, `055`) and problems with the mitral valve (`033`, `053`). 3 further females (4.61%) were excluded since they reported suffering from other dysfunctions affecting the **nervous system**, including thalassemia (`009`), anxiety (`020`), Crohn’s disease (`049`).
```{r }
# inclusion criteria variables
ic <- colnames(PrelQS)[which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")]

# cardiovascular dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$cvDysf=="Yes", c("ID","gender",ic)]

# other dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$otherDysf=="Yes", c("ID","gender",ic)]
```

<br>

#### MEDICATIONS (3)

Here, we show the No. of participants not invited to take part in the EMA protocol due to **medications** affecting either the *cardiovascular* or the *nervous* system. 3 participants (`017`, F; `029`, M; `033`, M) were excluded since they took **antidepressants** (Laroxyl, Venlafaxina, Remeron), one of which also suffered from a cardiovascular dysfunction.
```{r }
# cardiovascular dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$drugs=="Yes", c("ID","gender",ic)]
```

<br>

#### SMARTPHONE (1)

Here, we show the No. of participants not invited to take part in the EMA protocol due to **medications** affecting either the *cardiovascular* or the *nervous* system. Only 1 female (`051`) was excluded since she reported to not own a personal smarpthone.
```{r }
# cardiovascular dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$phone=="No", c("ID","gender",ic)]
```

<br>

#### OTHER REASONS (46)

The further 46 participants marked with `"OUT"` were not invited due to other reasons (e.g., calendar incompatibility, end of the study). Here, we inspect the data of such participants and we **remove them from the dataset**. We can see that these participants were mainly females (probably due to the higher No. of female compared to undergraduates participating to psychological studies, and our goal of having a balanced sample), with no other peculiarities compared to the rest of the sample.
```{r }
# showing participants that were not invited for other reasons
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & !(PrelQS$ID%in%c("OUT033","OUT053","OUT027","OUT055","OUT009","OUT020",
                                                       "OUT049","OUT017","OUT029","OUT051")),]
```

<br>

### 1.3.3. Data filtering

Here, we remove all participants marked as "OUT" from the `Prelqs` dataset.
```{r }
new <- PrelQS[!substr(PrelQS$ID,1,3)=="OUT",]
cat("Filtered",nrow(PrelQS)-nrow(new),"cases; new number of cases =",nrow(new))
PrelQS <- new
```

<br>

### 1.3.4. Flagged participants

Some of the **included participants reported suffering from dysfunctions or taking medications** that were considered irrelevant for the current study. Here, we better inspect those conditions (N = 9 and 13, respectively):

- **6** included participants (4 females, 2 males) reported suffering from **cardiovascular dysfunctions**: arrhythmia, tachycardia episodes, and bicuspid aortic valve

- **3** included participants (2 females, 1 male) reported suffering from **other dysfunctions**: hypothyroidism, asthma, and allergy

- **6** females reported taking **hormonal contraceptives**

- **8** participants took other medications including antihistamines (2 males, 5 females), and thyroid hormones (EUTIROX, 1 female)

```{r }
# cardiovascular dysfunctions
PrelQS[PrelQS$cvDysf=="Yes", c("ID","gender",ic)]

# other dysfunctions
PrelQS[PrelQS$otherDysf=="Yes", c("ID","gender",ic)]

# drugs
PrelQS[PrelQS$drugs=="Yes", c("ID","gender",ic)]
```

<br>

Although we considered such conditions as potentially irrelevant for the present study, some of them (especially cardiovascular and other dysfunctions) might play a role. Thus, we create the `dysfun` variable to flag cases of **included participants with cardiovascular and/or other dysfunctions**.
```{r }
# recoding dysfun variable (accounting for both cardiovascular and other dysfunctions)
PrelQS$dysfun <- 0
PrelQS[PrelQS$cvDysf=="Yes" | PrelQS$otherDysf=="Yes","dysfun"] <- 1

# recoding drugs variable
PrelQS$drugs <- gsub("Yes","1",gsub("No","0",PrelQS$drugs))

# removing unnecessary variables
PrelQS[,c("cvDysf","otherDysf","cvDysf.which","otherDysf.which","drugs.which","phone","phone.which")] <- NULL

# summary of dysfun and drugs
PrelQS[,c("dysfun","drugs")] <- lapply(PrelQS[,c("dysfun","drugs")],as.factor) # both as factor
summary(PrelQS[substr(PrelQS$ID,1,3)=="HRV",c("dysfun","drugs")])
```

<br>

### 1.3.5. Further excluded

**8 further participants were excluded** due to dropping-out during the EMA protocol (`009`, F; `059`, M), poor quality of physiological data (`015`, F; `046`, F; `066`, M) or technical problems with the mobile app (`011`, M; `027`, M; `029`, M). Here, we mark these participants as 'OUT'. 
```{r }
# marking 8 excluded participants as "OUT"
memory <- PrelQS
PrelQS$ID <- as.character(PrelQS$ID)
excl <- c("HRV009","HRV011","HRV015","HRV027","HRV029","HRV046","HRV059","HRV066")
PrelQS[PrelQS$ID%in%excl,"ID"] <- gsub("HRV","OUT",PrelQS[PrelQS$ID%in%excl,"ID"]) # marking excluded participants as "OUTXXX"
PrelQS$ID <- as.factor(PrelQS$ID)
PrelQS <- PrelQS[order(PrelQS$ID),]

# updating number of included participants
cat(nrow(PrelQS[substr(PrelQS$ID,1,3)=="HRV",]),"included participants out of",
    nrow(memory[substr(memory$ID,1,3)=="HRV",]),"invited participants")
```

<br>
<br>

# 2. ESM data

Here, the raw data collected with the **experience sampling method (ESM)** are read and saved in the `ESM` dataset, recoded, and filtered based on response rate and inclusion criteria.

## 2.1. Data reading

First, we read the JSON data files exported from the [Sensus Mobile app](https://predictive-technology-laboratory.github.io/sensus/) ([Xiong et al., 2016](#ref)). 

Specifically, the [Protocols created with Sensus](https://github.com/Luca-Menghini/vmHRV-selfRegulation/tree/main/1ESMmeasures/sensus/protocols) were configured to store the recorded .JSON data files in our private [AWS S3](https://aws.amazon.com/s3/) bucket, from which they were downloaded and stored in the `ESM/data` folder. From the Sensus app, we also exported the [Probe Definition files](https://github.com/Luca-Menghini/vmHRV-selfRegulation/tree/main/1ESMmeasures/sensus/probe), which we use to replace the *input IDs* (i.e., by default, each item is associated with an alphanumeic code) with the corresponding *input names* (i.e., the item labels that we set in the protocols). These files were saved in the `ESM/probe` folder.

The `readSurveyData` function is used to optimize the ESM data reading.

<details><summary>`readSurveyData`</summary>
<p>
```{r }
#' @title Read JSON data exported from the Sensus mobile app
#' @param data.path = character string indicating the full path to the folder where the JSON raw data are stored.
#' @param probe.definition = character string indicating the full path to the folder where Probe Definition JSON file(s) (from the Sensus mobile app: Protocol > Probe > Scripted Interaction > Share definition).
#' @param messages = logical value indicating whether the processing steps should be printed (default: TRUE).
readSurveyData <- function(data.path,probe.definition,messages=TRUE){ require(jsonlite)

  # 1. Reading data
  # .......................................
  options(digits.secs=3) # setting No. of digits
  paths = list.files(data.path,recursive=TRUE,full.names=TRUE,include.dirs=FALSE) # listing files in data path
  if(messages==TRUE){ cat("\n\nReading",length(paths),"data files from",data.path,"...")}
  var.names <- c("ParticipantId","Timestamp","InputId","Response","RunTimestamp","SubmissionTimestamp",
                 "ScriptName","ProtocolId","$type") # selecting variables of interest
  data <- as.data.frame(matrix(nrow=0,ncol=9)) # empty dataframe creation and population
  colnames(data) <- var.names
  for(path in paths){ 
    if(file.info(path)$size>0){ # only reading Datum files (i.e., containing ScriptDatum, which is sized > 0 Kb)
      new.data <- read_json(path,simplifyDataFrame=TRUE)
      if(class(new.data)=="data.frame" & !is.null(new.data$Response)){ # only keeping files with Response data
        if(class(new.data$Response)=="data.frame"){ # sometimes the Response column is read as dataframe
          new.data$Response <- as.character(new.data$Response$`$values`)}
        data <- rbind(data,new.data[var.names]) }}} 
  data <- data[!is.na(data$ParticipantId),] # only keeping data with ParticipantId information (i.e., participant identification)
  data <- data[!is.na(data$InputId),] # only keeping data with InputId information (i.e., item identification)
  names(data)[9] <- "os" # $type as OS (android or iOS) # mobile OS information
  data[,9] <- gsub("Sensus.Probes.User.Scripts.ScriptDatum, Sensus","",data[,9]) # removing unuseful information
  
  # 2. From Response Ids to Item labels (based on Probe Definition) 
  # ...............................................................
  if(!is.na(probe.definition)){ if(messages==TRUE){ cat("\n\nConverting ResponseId into InputId based on Probe Definition...")}
    # function to read Probe Definition files
    readProbe <- function(path){ probedefinition <- read_json(path,simplifyDataFrame=TRUE) # first probe definition file
      # reading input labels of the first inputGroup
      inputs <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[1]]$Inputs$`$values`[[1]]$Name
      infos <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[1]] # input labels
      PROTOCOL <- data.frame(protocolName=probedefinition$Protocol$Name,protocolId=probedefinition$Protocol$Id, # protocol name
                             scriptName=infos$Name,inputName=inputs,inputId=infos$Inputs$`$values`[[1]]$Id) # script name
      if(length(probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`)>1){ # reading the following inputGroups
        for(i in 2:length(probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`)){
          inputs <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[i]]$Inputs$`$values`[[1]]$Name
          infos <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[i]]
          PROTOCOL <- rbind(PROTOCOL,data.frame(protocolName=probedefinition$Protocol$Name,
                                                protocolId=probedefinition$Protocol$Id,
                                                scriptName=infos$Name,inputName=inputs,
                                                inputId=infos$Inputs$`$values`[[1]]$Id)) }}
      return(PROTOCOL) }
    paths = list.files(probe.definition,recursive=TRUE,full.names=TRUE,include.dirs=FALSE) # listing files in probe.definition path
    PROTOCOL <- readProbe(paths[1]) # reading the first Probe Definition file
    if(length(list.files(probe.definition))>1){ # reading the following Probe Definition files when > 1
      for(path in paths[2:length(paths)]){ PROTOCOL <- rbind(PROTOCOL,readProbe(path)) }}
    for(i in 1:nrow(data)){ 
      for(j in 1:nrow(PROTOCOL)){ if(!is.na(data[i,3]) & data[i,3]==PROTOCOL[j,5]){ data[i,3] <- as.character(PROTOCOL[j,4]) }}}}
  
  # 3. Cleaning and unlisting Response data
  # ...............................................................
  if(messages==TRUE){ cat("\n\nUnlisting Response data and removing system information...")}
  data$Response <- gsub("list","",data$Response) # cleaning categorical items from Sensus system info
  data$Response <- gsub(paste("c","\\(|\\)",sep=""),"",data$Response)
  data$Response <- gsub("\\(|\\)","",data$Response)
  data$Response <- gsub("\\[|\\]","",data$Response)
  data$Response <- gsub("\\$type` = \"System.Collections.Generic.List`1System.Object, mscorlib, mscorlib\", ","",data$Response)
  data$Response <- gsub("\\$values","",data$Response)
  data$Response <- gsub('``` = ', "",data$Response)
  data$Response <- gsub('\ ', "",data$Response)
  data$Response <- gsub('\"', "",data$Response)
  data$Response <- gsub('\"No\"', "No",data$Response)
  data$Response <- gsub('\"Sì\"', "Si",data$Response)
  if(class(data$Response)=="data.frame"){ data$Response <- as.character(data$Response$`$values`[[1]]) # unlisting Response column
    } else { data$Response <- as.character(data$Response) }
  
  # 4. Encoding time information
  # ...............................................................
  if(messages==TRUE){ cat("\n\nConverting Timestamp variables as POSIXct...")}
  timestamps <- c("Timestamp","RunTimestamp","SubmissionTimestamp") # timestamps variables
  data[,timestamps] <- lapply(data[,timestamps],function(x) as.POSIXct(x,format="%Y-%m-%dT%H:%M:%OS",tz="GMT"))
  
  # 5. Cleaning incomplete double responses
  # ...............................................................
  # in some cases, multiple responses are recorded with the same ParticipantId, RunTimeStamp, and InputId
  if(messages==TRUE){ cat("\n\nLooking for double responses...\n")}
  data$id.r <- paste(data$ParticipantId,data$RunTimestamp,data$InputId,sep="_") # ID-time-item identifier
  data$id.t <- paste(data$ParticipantId,data$RunTimestamp,sep="_") # ID-time identifier
  doubles <- levels(as.factor(data[duplicated(data$id.r),"id.t"])) # double responses (same ID-time-item identifier)
  ndoubles <- rep(length(doubles),2) # No. of double responses
  for(double in doubles){ subId <- levels(as.factor(data[data$id.t==double,"SubmissionTimestamp"])) # submission time
    if(length(subId)>1){ ndoubles[1] <- ndoubles[1] - 1
      maxDouble <- max(as.POSIXct(subId)) # when the SubmissionTimestamp is different -> splitting responses
      data[data$id.t==double,"RunTimestamp"] <- maxDouble - 61.20 # 2nd RunTimeStamp = Submission - 61.20 sec (median resp time) 
    } else { ndoubles[2] <- ndoubles[2] - 1
      data <- rbind(data[data$id.t!=double,], # when even the SubmissionTimestamp is different -> removing double resp.
                    data[data$id.t==double & !duplicated(data$id.r),]) }}
  cat(" - Removed",ndoubles[1],"double responses (same ID, InputName, RunTimestamp, & SubmissionTimestamp)\n",
      "- Splitted",ndoubles[2],"double responses with different SubmissionTimestamp\n",
      "  (in these cases, RunTimestamp is recomputed as SubmissionTimestamp - 61.20 sec (average response length)")
  
  # 6. Sorting columns and Reshaping
  # ...............................................................
  if(messages==TRUE){ cat("\n\nSorting columns and reshaping (one row per data entry)...")}
  colnames(data)[1] <- "ID" # selecting and sorting columns
  data <- data[,c("ID","os","ProtocolId","ScriptName","RunTimestamp","SubmissionTimestamp","InputId","Response","Timestamp")]
  data <- data[order(data$ID,data$Timestamp),] # sorting rows
  wide <- reshape(data[,1:ncol(data)-1],v.names=c("Response"),timevar=c("InputId"), # reshaping from long to wide (resp-by-resp)
                  idvar=c("RunTimestamp","SubmissionTimestamp"),direction=c("wide"),sep="")
  colnames(wide) <- gsub("Response","",gsub("\\-",".",colnames(wide))) # removing label "Response" from ResponseId
  wide <- wide[order(wide$ID,wide$RunTimestamp),] # sorting rows by participant ID and RunTimestamp
  # wide <- data.frame(cbind(wide[1:3],wide[ncol(data)],wide[5:ncol(data)-1])) # final sorting of rows and columns
  wide <- wide[order(wide$ID,wide$RunTimestamp),]
  row.names(wide) <- as.character(1:nrow(wide))
  if(messages==TRUE){ cat("\n\nRead",nrow(wide),"responses from",nlevels(as.factor(data$ID)),"participants.") }
  
  # 7. Processing response times
  # ...............................................................
  data$RT <- as.numeric(NA) # computing item response times (i.e., seconds between each response and the previous one)
  for(i in 2:nrow(data)){ if(data[i,"ID"]==data[i-1,"ID"] & data[i,"RunTimestamp"]==data[i-1,"RunTimestamp"]){
    data[i,"RT"] <- difftime(data[i,"Timestamp"],data[i-1,"Timestamp"],units="secs")}}
  data$RT.tot <- as.numeric(NA) # computing survey total response time (i.e., last - first response of each survey)
  data$survey <- as.factor(paste(data$ID,data$RunTimestamp,sep="_"))
  for(survey in levels(data$survey)){ data[data$survey==survey,"RT.tot"] <-
    difftime(data[data$survey==survey,"SubmissionTimestamp"][1],
             data[data$survey==survey,"Timestamp"][1],units="secs")}
  rt <- reshape(data[,c("ID","RunTimestamp","SubmissionTimestamp","RT.tot","InputId","RT")], # reshaping
                v.names=c("RT"),timevar=c("InputId"),idvar=c("RunTimestamp","SubmissionTimestamp"),direction=c("wide"),sep="")
  colnames(rt)[5:ncol(rt)] <- gsub("RT","",colnames(rt)[5:ncol(rt)]) # removing label "Timestamp" from ResponseId
  
  # 7. Returning processed dataset and response time
  # ...............................................................
  return(list(wide,rt))}
```
</p></details>

<br>

Here, we use the `readSurveyData` function (depicted above) to read and preliminary recode the .JSON data files, and to merge them into the `ESM` dataset. In one single case, the JSON string contains (illegal) UTF8 byte-order-mark.
```{r }
# data reading, encoding and saving
ESM <- readSurveyData(data.path="ESM/data",probe.definition="ESM/probe")
ESM.RT <- ESM[[2]]
ESM <- ESM[[1]]
```

<br>

We also integrate the `ESM` dataset with **4 additional responses** that were sent via instant messages (i.e., screenshot of the responses) due to technical problems. These were stored in the `screenshotX4.rda` file.
```{r }
load("ESM/screenshot_surevyMalfunctioning/screenshotX4.RData") # loading additiona responses
cat("Adding",nrow(screenshotX4),"additional responses sent via instant messages") # N = 4
ESM <- rbind(ESM,screenshotX4) # adding additional responses to the ESM dataset
```

<br>

Finally, we import the **"baseline surveys"** data (i.e., ESM data collected in the lab at the beginning of each day, using [Google Forms](https://www.google.com/intl/it/forms/about/)), exported from Google Form as a .CSV file. Some recoding procedures will be necessary before merging the `baseline` and the `ESM` datasets.
```{r }
# reading raw dataset exported from Google Forms
baseline <- read.csv("baseline/Baseline.csv",header=TRUE)
cat("Reading",nrow(baseline),"responses to the baseline questionnaire from",
    nlevels(as.factor(baseline$Nome.utente)),"participants")
```

<br>

## 2.2. Data recoding

Then, we recode the `ESM` and `baseline` variables to be used for the analyses, and we merge the two datasets. 

The following packages and functions are used to optimize the ESM data recoding:
```{r warning=FALSE,message=FALSE}
library(lubridate); library(plyr); library(scales)
```

<details><summary>`within.day.adjust`</summary>
<p>
```{r }
within.day.adjust <- function(data){
  
  # Creating within.day.indicator (dafault = 0)
  data$within.day <- NA
  
  # adjusting within.day based on the scheduled times
  for(i in 1:nrow(data)){
    if(strftime(data[i,"RunTimestamp",], # survey 1 between 11:20 (- 10 min error) and 11:40 (up to 12.00 + 10min error)
                format="%H:%M:%S")>strftime("1970-01-01 11:10:00",
                                            format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                          format="%H:%M:%S")<strftime("1970-01-01 12:10:00",
                                                                                                      format="%H:%M:%S")){
      data[i,"within.day"] = 1 } 
    else if(strftime(data[i,"RunTimestamp",], # survey 2 between 12:30 (- 10min error) and 12:50 (up to 13:10 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 12:20:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 13:20:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 2 } 
    else if(strftime(data[i,"RunTimestamp",], # survey 3 between 13:40 (- 10min error) and 14:00 (up to 14:20 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 13:30:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 14:30:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 3} 
    else if(strftime(data[i,"RunTimestamp",], # survey 4 between 14:50 (- 10min error) and 15:10 (up to 15.30 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 14:40:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 15:40:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 4 } 
    else if(strftime(data[i,"RunTimestamp",], # survey 5 between 16:00 (- 10min error) and 16:20 (up to 16:40 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 15:50:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 16:50:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 5 }}
  
  # sorting dataset based on ID, day.of.week and within.day
  data <- data[order(data$ID,data$day.of.week,data$within.day),]
  data$within.day <- as.factor(data$within.day)
  rownames(data) <- 1:nrow(data)
  cat("Creating/Adjusting within.day variable...\n Printing summary of cases:\n")
  print(summary(data$within.day))
  return(data) }
```
</p></details>

Creates or adjusts the `within.day` variable based on the scheduled temporal windows used in the current study.

<details><summary>`varsRecoding`</summary>
<p>
```{r }
varsRecoding <- function(data){

  # ID and os as factors
  data[,c("ID","os")] <- lapply(data[,c("ID","os")],as.factor)
  
  # Recoding item scores and reversed items
  ratings <- colnames(data)[c(which(colnames(data)=="t3.nervoso.tranquillo"),  # recoding ratings as integer
                              which(colnames(data)=="e3.affaticato.fresco"):which(colnames(data)=="intensity.NE"),
                              which(colnames(data)=="v1.male.bene"):which(colnames(data)=="v3.positivo.negativo"))]
  data[,ratings] <- lapply(data[,ratings],as.integer)
  data$v3.positivo.negativo <- 8 - data$v3.positivo.negativo # HEDONIC TONE = positive
  colnames(data)[which(colnames(data)=="v2.soddisfatto.insoddisfatto")] <- "v2.insoddisfatto.soddisfatto" # changing wrong label
  data$t1.rilassato.teso <- 8 - data$t1.rilassato.teso # CALMNESS = positive
  data$e2.pieno.privodenergia <- 8 - data$e2.pieno.privodenergia # ENERGETIC AROUSAL = positive

  # Recoding confounders
  data$alcohol <- as.factor(gsub("No","0",gsub("Sì","1",data$alcohol))) # alcohol (from yes/no --> 1/0)
  data$Activity <- as.factor(gsub("Leggerasedutiocoricatiperlamaggiorpartedeltempo","0", # physical activity level (0 = seated)
                                  gsub("Moderatacamminata,farelescale","1", # (1 = moderate)
                                       gsub("Intensaattivitàsportiva,palestra,corsaodiverserampediscale","2", # (2 = vigorous)
                                            data$Activity))))
  data$People.rec <- as.factor(gsub("No,erodasola","0",gsub("No,erodasolo","0", # (0=alone)
                                gsub("Sì,manonsonostatadisturbata","1",gsub("Sì,manonsonostatadisturbato","1", # (1=not interact)
                                     gsub("Sì,esonostataunpo'disturbata","2",gsub("Sì,esonostataunpo'disturbato","2", # (2=disturb)
                                          gsub("Sì,esonostatainterrotta","3",gsub("Sì,esonostatainterrotto","3", #(3 = interrupted)
                                               data$People.rec)))))))))
  data$ProtocolId <- as.factor(gsub("ProtocolStudent","",data$ProtocolId)) # from protocol ID to gender
  colnames(data)[which(colnames(data)=="ProtocolId")] <- "gender"
  data[,c("smoke","coffe")] <- lapply(data[,c("smoke","coffe")],as.factor) # remaining confounders as factor 0/1
  
  # Sorting and renaming columns
  data <- data[,c("ID","os","gender","day.of.week","within.day","RunTimestamp", # response information
                  "v1.male.bene","v2.insoddisfatto.soddisfatto","v3.positivo.negativo", # hedonic tone
                  "t1.rilassato.teso","t2.agitato.calmo","t3.nervoso.tranquillo", # calmness
                  "e1.stanco.sveglio","e2.pieno.privodenergia","e3.affaticato.fresco", # energetic arousal
                  "PE","intensity.PE","NE","intensity.PE", # emotional events
                  "smoke","coffe","alcohol","Activity","People.rec")] # confounders
  colnames(data)[7:ncol(data)] <- c("v1","v2","v3","t1","t2","t3","f1","f2","f3", # renaming columns
                                    "PE","PE.int","NE","NE.int","smoke","coffe","alcohol","activity","people")
  
  # final sorting and returning data
  data$sort <- as.numeric(substr(data$ID,4,6))
  data <- data[order(data$sort,data$RunTimestamp),] # sorting by ID and timestamp
  data$sort <- NULL # removing sort column
  return(data) }
```
</p></details>

Recodes slider and multiple-choice responses collected with the Sensus protocols used in the study, sort the columns and fix problems due to Daylight Time changes.

<details><summary>`bslRecoding`</summary>
<p>
```{r }
bslRecoding <- function(data){ 
  
  # renaming and sorting columns
  colnames(data) <- c("RunTimestamp","ID","v1","t1","f1","v2","t2","f2","v3","t3","f3",
                          "PE","PE.int","NE","NE.int","Sleep1","Sleep2","Sleep3","daybefore.ex","drugs","drugs.which","day.of.week","within.day")
  data <- data[,c("ID","day.of.week","within.day","RunTimestamp",
                  "v1","v2","v3","t1","t2","t3","f1","f2","f3", # renaming columns
                  "PE","PE.int","NE","NE.int","Sleep1","Sleep2","Sleep3","daybefore.ex","drugs","drugs.which")]

  # ID as factor
  data$ID <- as.factor(data$ID)
  
  # recoding item scores
  data$v2 <- 8 - data$v2 # hedonic tone = positive
  data$v3 <- 8 - data$v3
  data$t2 <- 8 - data$t2 # t.arousal = negative
  data$t3 <- 8 - data$t3
  data$f2 <- 8 - data$f2 # e.arousal = positive
  data$Sleep2 <- 8 - data$Sleep2 # sleep quality = positive
  data$Sleep3 <- 8 - data$Sleep3

  # other confounding variables
  data$drugs <- as.factor(gsub("Sì","1",gsub("No","0",data$drugs))) # drugs
  data$daybefore.ex <- as.factor(gsub("Sì","1",gsub("No","0",data$daybefore.ex))) # intense exercise on the day before

  # sorting by timestamp
  data <- data[order(data$ID,data$RunTimestamp),] 
  
  return(data)}
```
</p></details>

<br>

### 2.2.1. ID recoding

As a first step, we **recode participants' identification codes** `ID` (currently corresponding to their e-mail addresses) using the "HRVXXX" format (e.g., from '[john.smith\@gmail.com](mailto:john.smith@gmail.com){.email}' to 'HRV001'). This is done with the `prel.qs_IDrecode()` and the `participantID_recoding_baseline()`  functions. Again, since the participants' e-mail addresses are confidential, both the function and the original dataset are not showed.
```{r }
# loading and using the function to recode the ID variable
source("participantID_recoding.R"); source("participantID_recoding_baseline.R")
(ESM <- participantID_recoding(ESM))[1:3,] # recoding ESM IDs and showing first three rows
colnames(baseline)[2] <- "ID"
(baseline <- participantID_recoding_baseline(baseline))[1:3,] # recoding baseline IDs and showing first three rows
```

<br>

### 2.2.2. Time recoding

Here, we process the **timestamps** (i.e., temporal variables) in both the `EMA` and the `baseline` datasets.

#### 2.2.2.1. Timestamps

First, we convert the included timestamps **from character to POSIXct** (the R format for dates and times), using the "GMT" time zone (note that data were collected in the CET/CEST region, but GMT is easier to handle).
```{r }
# converting temporal variables as POSIXct in the ESM dataset
ESM[,c("RunTimestamp","SubmissionTimestamp")] <- # no need to specify format since they already are POSIXct
  lapply(ESM[,c("RunTimestamp","SubmissionTimestamp")],function(x) as.POSIXct(x,tz="GMT"))

# converting baseline variables as POSIXct in the baseline dataset
colnames(baseline)[which(colnames(baseline)=="Informazioni.cronologiche")] <- "RunTimestamp"
baseline$RunTimestamp <- as.POSIXct(baseline$RunTimestamp,format="%Y/%m/%d %I:%M:%S %p",tz="GMT")
```

<br>

Second, we inspect the data for cases of **wrongly encoded timestamps**, considering that data collection started on November 6th, 2018 and ended on November 21th, 2019. Only two cases are detected, whose `RunTimestamp` was recorded with the wrong year.
```{r }
# min and max RunTimestamp
c(min(ESM$RunTimestamp),max(ESM$RunTimestamp)) # ESM: unexpected values < November 2018
c(min(baseline$RunTimestamp),max(baseline$RunTimestamp)) # baseline: ok

# only two cases with RunTimestamp < November 2018 and no SubmissionTimestamp
ESM[ESM$RunTimestamp < as.POSIXct("2018-11-06") | ESM$RunTimestamp > as.POSIXct("2019-11-22"),
    c("ID","RunTimestamp","SubmissionTimestamp")]
# plotting all timestamps from these two participants: isolated point recorded as it was collected one year before
grid.arrange(ggplot(ESM[ESM$ID=="HRV046",],aes(RunTimestamp)) + geom_histogram(bins=30) + ggtitle("HRV046"),
             ggplot(ESM[ESM$ID=="HRV048",],aes(RunTimestamp)) + geom_histogram(bins=30) + ggtitle("HRV048"))

# correcting wrongly encoded RunTimestamps (adding 1year) and SubmissionTimestamps (RunTimestamp + 61.20 sec)
ESM[substr(ESM$RunTimestamp,1,7)=="2018-01","SubmissionTimestamp"] <-
  ESM[substr(ESM$RunTimestamp,1,7)=="2018-01","RunTimestamp"] + 61.20
ESM[substr(ESM$RunTimestamp,1,7)=="2018-01",c("RunTimestamp","SubmissionTimestamp")] <-
  ESM[substr(ESM$RunTimestamp,1,7)=="2018-01",c("RunTimestamp","SubmissionTimestamp")] + 24*60*60*365
c(min(ESM$RunTimestamp),max(ESM$RunTimestamp)) # sanity check: min and max matching with the data collection period

# 2 further cases with missing SubmissionTimestamp (replacing with RunTimestamp + 61.20 sec)
ESM[is.na(ESM$SubmissionTimestamp),c("ID","RunTimestamp","SubmissionTimestamp")]
ESM[is.na(ESM$SubmissionTimestamp),"SubmissionTimestamp"] <- ESM[is.na(ESM$SubmissionTimestamp),"RunTimestamp"] + 61.20
```

<br>

#### 2.2.2.2. Daylight saving time

Here, we **adjust the temporal coordinates** accounting for **daylight saving time**. Indeed, whereas timestamps were recorded based on the internal clock of participants' mobile phones, which automatically synchronizes based on time changes, such time shifts got lost in the data exporting, resulting in **one-hour shifts during standard time**. In Italy, during the data collection period, daylight saving time was applied from March 31th to October 27th 2019. Here, we visually inspect the timestamp of the first response for both the `EMA` and the `baseline` datasets.
```{r fig.width=8,fig.height=3}
# ESM: computing and plotting time without date
ESM$daytime <- as.POSIXct(paste(hour(ESM$RunTimestamp),minute(ESM$RunTimestamp)),format="%H %M",tz="GMT")
ESM$daylight <- "standard"
ESM[ESM$RunTimestamp > as.POSIXct("2019-04-01") & ESM$RunTimestamp < as.POSIXct("2019-10-27"),"daylight"] <- "daylight"
ggplot(ESM[!duplicated(ESM$ID),],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("ESM$RunTimestamp of the first response for each participant")

# baseline: computing and plotting time without date
baseline$daytime <- as.POSIXct(paste(hour(baseline$RunTimestamp),minute(baseline$RunTimestamp)),format="%H %M",tz="GMT")
baseline$daylight <- "standard"
baseline[baseline$RunTimestamp>as.POSIXct("2019-04-01") & baseline$RunTimestamp<as.POSIXct("2019-10-27"),"daylight"]<-"daylight"
ggplot(baseline[,],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("baseline$RunTimestamp")
```

<br>

*Comments:*

- in the `EMA` dataset, the responses recorded during **standard time** were encoded with **one hour less** than the actual time (i.e., the first questionnaire was scheduled at 11:30, whereas most responses were recorded at 10:30), whereas those recorded during **daylight saving time** showing **one additional hour less** than the actual time (i.e., most responses recorded at 9:30)

- in the `baseline` dataset, the responses recorded during **daylight saving time** were recorded with **one hour less** than the actual time (i.e., baseline questionnaires were administered around 10:30, whereas most responses are around 9:30), whereas those recorded during **standard time** look fine

<br>

Here, we adjust the temporal coordinates in both datasets according to the observed time shifts.
```{r fig.width=8,fig.height=3}
# adding 1 hour to all ESM timestamps
ESM[,c("RunTimestamp","SubmissionTimestamp")] <- ESM[,c("RunTimestamp","SubmissionTimestamp")] + 1*60*60
# adding 1 further hour to ESM timestamps during daylightsaving time
ESM[ESM$daylight=="daylight",c("RunTimestamp","SubmissionTimestamp")] <- 
  ESM[ESM$daylight=="daylight",c("RunTimestamp","SubmissionTimestamp")] + 1*60*60

# adding 1 hour to baseline timestamps during daylightsaving time
baseline[baseline$daylight=="daylight","RunTimestamp"] <- baseline[baseline$daylight=="daylight","RunTimestamp"] + 1*60*60

# ESM: computing and plotting time without date (NOW OK)
ESM$daytime <- as.POSIXct(paste(hour(ESM$RunTimestamp),minute(ESM$RunTimestamp)),format="%H %M",tz="GMT")
ggplot(ESM[!duplicated(ESM$ID),],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("ESM$RunTimestamp of the first response for each participant")

# baseline: computing and plotting time without date (NOW OK)
baseline$daytime <- as.POSIXct(paste(hour(baseline$RunTimestamp),minute(baseline$RunTimestamp)),format="%H %M",tz="GMT")
ggplot(baseline[,],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("baseline$RunTimestamp of the first response for each participant")

# removing daytime and daylight columns
ESM$daytime <- ESM$daylight <- baseline$daytime <- baseline$daylight <- NULL
```

<br>

#### 2.2.2.3. Days & within.day

Finally, we create two variables to better organize the temporal structure of the datasets: 

- `day.of.week` indicating the weekday indicator (i.e., 1 = Tuesday, 2 = Wednesday, 3 = Thursday)

- `within.day` consisting in a row identifier within each day. For `baseline` responses, we set `within.day = 0`
```{r }
# creating indicator for the weekday 
ESM$day.of.week <- as.POSIXlt(ESM$RunTimestamp)$wday - 1
baseline$day.of.week <- as.POSIXlt(baseline$RunTimestamp)$wday - 1
levels(as.factor(c(levels(as.factor(ESM$day.of.week)),levels(as.factor(baseline$day.of.week))))) # sanity check: ok

# creating row identifier within each day
ESM <- ddply(ESM,c("ID","day.of.week"),transform,within.day=seq_along(day.of.week))
baseline$within.day <- 0 # assigning zero to the baseline questionnaires

# showing data structure
ESM[,c("ID","day.of.week","within.day","RunTimestamp")]
```

<br>

However, the method used above to encode the `within.day` variable counts the survey as they are received (i.e., first, second, third), and not as they were *scheduled* (i.e., based on the `RunTimestamp` variable), neglecting missing responses. 

Here, we use the `within.day.adjust()` function showed at the beginning of section 2.2 to recode the `within.day` variable accounting for the following **scheduled temporal windows** and the **expiration time** (i.e., after 20 minutes), and we add **10 extra minutes** before and after the following time limits to account for the variability among devices:

-   *0* = 09:45 \~ 11:00 (*baseline survey*)

-   *1* = 11:20 - 11:40 + 20 min (up to 12:00)

-   *2* = 12:30 - 12:50 + 20 min (up to 13:10)

-   *3* = 13:40 - 14:00 + 20 min (up to 14:20)

-   *4* = 14:50 - 15:10 + 20 min (up to 15:50)

-   *5* = 16:00 - 16:20 + 20 min (up to 16:40)

Here, we apply the function and we correct one case of missing `within.day` (i.e., `RunTimestamp` = 17:40; recorded as `within.day = 5`).
```{r message=FALSE,warning=FALSE}
# adjusting within.day based on scheduled time windows
ESM <- within.day.adjust(ESM)

# correcting 1 missing case (17:40 -> within.day = 5)
ESM[is.na(ESM$within.day),"within.day"] <- 5

# showing data structure
ESM[,c("ID","day.of.week","within.day","RunTimestamp")]
```

<br>

### 2.2.3. Other variables

Finally, we use the `varsRecoding()` and the `bslRecoding()` functions showed at the beginning of section 2.2 to recode the remaining variables (i.e., renaming and reversing item scores, recoding categorical variables, sorting and renaming columns) in the `ESM` and the `baseline` datasets, respectively.
```{r }
# recoding EMA dataset
ESM <- varsRecoding(ESM)

# recoding baseline dataset
baseline <- bslRecoding(baseline)
```

<br>

Here, we conduct a series of sanity checks by inspecting the summary and plots of each variable. In the `ESM` dataset, we can see that 11 cases from the same participant `HRV099` have both `coffe` and `alcohol = "NULL"`, whereas the values of the other confounders look fine. Since this is likely due to unanswered items because the condition was not met, we recode those cases with `coffe` and `alcohol = 0`. Moreover, we can note that in some cases participants selected more than one option for the `people` variable. In these cases, we recode the score in a conservatory way, that is by assigning the higher score.
```{r }
# inspecting summary of ESM variables
summary(ESM)

# cases with coffe or alcohol = "NULL"
ESM[!is.na(ESM$coffe) & ESM$coffe=="NULL",c(1,20:ncol(ESM))]
ESM[!is.na(ESM$coffe) & ESM$coffe=="NULL",c("smoke","coffe")] <- "0"
ESM[,c("smoke","coffe")] <- lapply(lapply(ESM[,c("smoke","coffe")],as.character),as.factor)

# inspecting summary of people
summary(ESM$people)
ESM$people <- as.factor(gsub("0,1","0", # replacing multi-option responses with the higher selected score
                             gsub("0,1,2","2",
                                  gsub("0,2","2",
                                       gsub("1,0","1",
                                            gsub("2,1","2",
                                                 gsub("2,3","3",
                                                      gsub("3,0","3",
                                                           gsub("3,1","3",ESM$people)))))))))

# re-inspecting recoded variables (no more "NULL" and multi-choice responses)
summary(ESM[,c("smoke","coffe","people")])

# inspecting summary of baseline variables (all look fine)
summary(baseline)
```

<br>

#### 2.2.3.1. Binary confounders

From the summary above, we can see that some categorical confounders have only a very small number of ratings for certain levels (i.e., `smoke` has only 11 cases rated as "2" or higher, `coffe` has only 8 cases rated as "2", activity has only 29 cases rated as "2", people has only 3 cases rated as "3"). Here, we simplify such variables by **dichotomizing each of them** (i.e., 0/1).
```{r fig.width=8,fig.height=1.5}
# plotting frequency of cases in each level of each confounder
par(mfrow=c(1,4))
for(conf in c("smoke","coffe","activity","people")){ plot(ESM[,conf],main=conf) }

# smoke: 2 and 5 --> 1 (i.e., "1+ cigarettes")
ESM$smoke <- as.factor(gsub("2","1",gsub("5","1",ESM$smoke)))

# coffe: 2 --> 1 (i.e., "1+ coffes")
ESM$coffe <- as.factor(gsub("2","1",ESM$coffe))

# activity: 2 --> 1 (i.e., "moderate/intense physical activity")
ESM$activity <- as.factor(gsub("2","1",ESM$activity))

# people: 1 --> 0 (i.e., "alone or with some people not disturbing"); 2 and 3 --> 1 (i.e., "someone disturbing or interrupting the recording")
ESM$people <- as.factor(gsub("3","1",gsub("2","1",gsub("1","0",ESM$people))))

# re-plotting
par(mfrow=c(1,4))
for(conf in c("smoke","coffe","activity","people")){ plot(ESM[,conf],main=conf) }

# re-summarizing
summary(ESM[,c("smoke","coffe","activity","people")])
```

<br>

### 2.2.4. Data merging

Finally, we can join the `baseline` dataset with the `ESM` dataset.
```{r fig.width=8,fig.height=1.5}
# merging and fixing the last issues
ESM$within.day <- as.integer(as.character(ESM$within.day)) # within.day as integer for compatibility
new <- rbind.fill(ESM,baseline) # merging datasets
cat("sanity check:",nrow(new)==(nrow(ESM)+nrow(baseline))) # sanity check
ESM <- new[order(new$ID,new$day.of.week,new$within.day),] # sorting by ID, day.of.week, and within.day
rownames(ESM) <- 1:nrow(ESM)

# showing merged dataset (everithing looks fine)
ESM
```

<br>

## 2.3. Cleaning & filtering

Here, we clean and filter the data based on the inclusion criteria. The following packages and functions are used to optimize data filtering:
```{r warning=FALSE,message=FALSE}
library(plyr); library(knitr); library(reshape2); library(ggplot2); library(gridExtra); library(careless)
```

<details><summary>`respRate`</summary>
<p>
```{r }
respRate <- function(data){
  
  # scheduled No. of surveys (1 baseline + 5 ordinary X 3 days)
  tot <- (1 + 5) * 3
  bsl <- 1 * 3
  ord <- 5 * 3
  
  # computing No. of responses and response rate
  data$ID <- as.factor(as.character(data$ID))
  rr <- data.frame(ID=levels(data$ID),tot=as.numeric(table(data$ID))) # No. of responses per participant
  rr$tot.RR <- round(100*rr$tot/tot) # total response rate
  rr$bsl <- as.numeric(table(data[data$within.day==0,"ID"])) # No. of responses to the baseline surveys
  rr$bsl.RR <- round(100*rr$bsl/bsl) # baseline response rate
  rr$ord <- as.numeric(table(data[data$within.day!=0,"ID"])) # No. of responses to the ordinary surveys
  rr$ord.RR <- round(100*rr$ord/ord) # ordinary response rate
  
  # function to print response rate info
  rr.message <- function(dat,what){
    cat("\n\n",what,"participants: N =",nrow(dat),
      "\n -",sum(dat$tot),"responses, from",min(dat$tot),"to",max(dat$tot),
      ", mean =",round(mean(dat$tot),2),"sd =",round(sd(dat$tot),2),
      ", mean perc =",round(mean(dat$tot.RR),2),"% sd =",round(sd(dat$tot.RR),2),"%",
      "\n  -",sum(dat$bsl),"baseline, from",min(dat$bsl),"to",max(dat$bsl),
      ", mean =",round(mean(dat$bsl),2),"sd =",round(sd(dat$bsl),2),
      ", mean perc =",round(mean(dat$bsl.RR),2),"% sd =",round(sd(dat$bsl.RR),2),"%",
      "\n  -",sum(dat$ord),"ordinary, from",min(dat$ord),"to",max(dat$ord),
      ", mean =",round(mean(dat$ord),2),"sd =",round(sd(dat$ord),2),
      ", mean perc =",round(mean(dat$ord.RR),2),"% sd =",round(sd(dat$ord.RR),2),"%") }
  rr.message(rr[substr(rr$ID,1,3)=="HRV",],what="Included") # included participants
  rr.message(rr[substr(rr$ID,1,3)!="HRV",],what="Excluded") # excluded participants
  
  return(rr)} # returning response rate dataset
```
</p></details>

Compute data frame of response rate values and print summary information on response rate.

<details><summary>`plotResp`</summary>
<p>
```{r }
plotResp <- function(data,id,items){
  long <- melt(data[,c("id",items)],id) # one row per item
  long <- long[order(long$id,long$variable),]
  ggplot(long,aes(x=value,y=variable)) + geom_vline(xintercept=4,lwd=0.5,lty=2) + geom_point() + xlim(1,7) +
  facet_wrap(id) + theme(text=element_text(size=8)) }
```
</p></details>

Plot pattern of responses to mood items for a subsample of responses.

<br>

First, we inspect the original number of respondents and observations in the `ESM` dataset. We can see that the questionnaires were completed by a total of **104 respondents**, one less than the 105 participants that were actually invited to participate to the daily protocol. This is because one participant `HRV009` because she dropped out without activating the app.
```{r }
cat("Original No. of responses to the ESM/baseline surveys =",nrow(ESM),"from",nlevels(ESM$ID),"participants")
```

<br>

### 2.3.1. Double responses

First, we inspect and remove cases of **double responses** with the same `ID` and `RunTimestamp` values (N = 10). In these cases, only the first response is included with the exception of `HRV007` on day 1 and `HRV073` on day 2, because of missing values in the first but not in the second response.
```{r }
# detecting double responses
ESM$idday <- paste(ESM$ID,ESM$RunTimestamp,sep="_")
cat(nrow(ESM[duplicated(ESM$idday),]),"cases of double responses (same ID and RunTimestamp)")
kable(ESM[ESM$idday%in%ESM[duplicated(ESM$idday),"idday"],
    c("ID","day.of.week","within.day","RunTimestamp","v1","v2","v3","t1","t2","t3","NE.int","smoke","coffe","alcohol","activity")])

# excluding second response for HRV007 (day 1) and HRV073 (day 2)
memory <- ESM
ESM <- ESM[!(ESM$idday=="HRV007_2018-11-13 16:18:32.769" & is.na(ESM$activity)),]
ESM <- ESM[!(ESM$idday=="HRV073_2019-05-08 13:42:51.806" & is.na(ESM$activity)),]

# excluding all remaining double responses
ESM <- ESM[!duplicated(ESM$idday),]

# removing double responses
cat("Removed",nrow(memory)-nrow(ESM),"double responses")
```

<br>

Then, we use the same procedure to inspect and remove cases of **double responses** with the same `ID`, `day.of.week` and `within.day` values. We can see 10 further cases of double responses. Since no missing data are included in any of these cases, we can just filter the latest response of each couple.
```{r }
# detecting double responses
ESM$idday <- paste(ESM$ID,ESM$day.of.week,ESM$within.day,sep="_")
cat(nrow(ESM[duplicated(ESM$idday),]),"cases of double responses (same ID and RunTimestamp)")
kable(ESM[ESM$idday%in%ESM[duplicated(ESM$idday),"idday"],
    c("ID","day.of.week","within.day","RunTimestamp","v1","v2","v3","t1","t2","t3","NE.int","smoke","coffe","alcohol","activity")])

# excluding double responses
memory2 <- ESM
ESM <- ESM[!duplicated(ESM$idday),]

# removing double responses
cat("Removed further",nrow(memory2)-nrow(ESM),"double responses")
```

<br>

Thus, we removed a total of 20 double responses
```{r }
# removing double responses
cat("Removed a total of",nrow(memory)-nrow(ESM),"double responses; new number of responses =",nrow(ESM))
```

<br>

### 2.3.2. Inclusion criteria

Second, we inspect the `daily.drugs` and the `drugs.what` variable (i.e., drugs/medications different than those usually taken that were assumed on that morning) considering the inclusion criteria of the study (i.e., not taking medications affecting the nervous or the cardiovascular system). 

Only 14 cases are present in the dataset, of which 7 (4 participants) concerned antiinflammatory medications (Antiinfiammatorio, moment, buscofen), 1 case concerned antibiotics (fosfomicina), 1 case concerned analgesics (paracetamolo), and 3 cases (1 participant) concerned cough syrup (sciroppo per la tosse). None of these cases contrast with our inclusion criteria, but they will be taken into account later, as they can influence some self-reported and physiological variables.
```{r }
ESM[substr(ESM$ID,1,3)=="HRV" & ESM$within.day==0 & ESM$drugs==1,c("ID","drugs.which")]
```


<br>

### 2.3.3. Further excluded

Here, we mark the **8 participants that were excluded** for the reasons specified in section 1.3.4. As in that section, such participants are marked as ‘OUT’. As noted above, participant `HRV009` was even not included in the `ESM` dataset because she dropped out before activating the app. Thus, from the initial 104 participants, the number of excluded participants is 7 (6.73%%), whereas the total number of included participants is 97 (93.27%).
```{r }
# marking 8 excluded participants as "OUT"
memory <- ESM
ESM$ID <- as.character(ESM$ID)
excl <- c("HRV009","HRV011","HRV015","HRV027","HRV029","HRV046","HRV059","HRV066")
ESM[ESM$ID%in%excl,"ID"] <- gsub("HRV","OUT",ESM[ESM$ID%in%excl,"ID"]) # marking excluded participants as "OUTXXX"
ESM$ID <- as.factor(ESM$ID)
ESM <- ESM[order(ESM$ID,ESM$RunTimestamp),]

# updating number of included participants
cat(nrow(ESM[substr(ESM$ID,1,3)=="HRV",]),"included observations from",
    nlevels(as.factor(as.character(ESM[substr(ESM$ID,1,3)=="HRV","ID"]))),"participants out of",
    nrow(memory),"total observations from",nrow(memory[!duplicated(memory$ID),]),"invited participants")
```

<br>

### 2.3.4. Flagged cases

Although we did not applied inclusion criteria based on further variables than those specified in section 1.3.2, here we consider the **response rate** and the **coherence of the responses** as further criteria to flag those cases that might bias the results.

#### 2.3.4.1. Response rate

First, we inspect the response rate by using the `respRate` function showed at the beginning of section 2.3. Considering included participants, we can see that **all ‘baseline’ surveys** (i.e., those administered in the lab) were responded each day by each participant, whereas a some **missing data were present in 'ordinary surveys'** (i.e., those administered with the mobile app). Overall, the mean response rate to 'ordinary' surveys was relatively high (**90%**) with no participants showing response rates < 60%. Thus, we see **no reasons for excluding or flagging further participants** based on response rate. In contrast, excluded participants showed substantially lower response rates.
```{r }
rrate <- respRate(ESM)
```

<br>

In addition to counting the number of received surveys, we must also consider **missing responses within each survey**. Indeed, a number of surveys was incomplete **due to technical problems** with the app. We can notice that missing responses **mainly concern items on confounder variables** (i.e., activity, alcohol, coffee, smoke and people), which were administered in the last part of each survey and showed from 49 to 59 missing values (about 4%). In contrast, missing data are only about the 2-3% for items measuring stressful events (from 26 to 35), and less than 2% for itmes measuring mood (from 0 to 2).
```{r }
miss <- sapply(ESM[ESM$within.day!=0,1:which(colnames(ESM)=="people")], function(x) sum(is.na(x))) # counting missing
miss <- data.frame(item=names(miss),nMiss=as.numeric(miss), # missing rate
                   percMiss=round(100*as.numeric(miss)/nrow(ESM[ESM$within.day!=0,])))
kable(miss) # showing No. and percentage of missing cases
```

<br>

Here, we **remove further 14 surveys (1.0%)** due to missing data in almost all items (i.e., those data entries with missing data in the first items), of which only 7 (0.5%) were entered by included participants
```{r }
# showing 14 cases with missing response to item v3
ESM[is.na(ESM$v3),c(1,which(colnames(ESM)=="v1"):ncol(ESM)),]

# excluding 14 cases with missing response to item v3
ESM <- ESM[!is.na(ESM$v3),]
```

<br>

#### 2.3.4.2. Careless responses 

Here, we flag participants and/or single responses that might be considered as **careless respondents/responses**. 

##### Indicators and procedure {.tabset .tabset-fade .tabset-pills}

Following [Curran (2016)](#ref), we consider multiple indicators of potentially careless responses including:

1. Response times (both item-specific and total)

2. Response consistency (long string proportions)

3. Multivariate outlier patterns (Mahalanobis distance)

4. Response inconsistency (psychometric synonyms)

For each indicator, the numbers between brackets indicates the corresponding number of flagged responses/participants.

###### INDICATORS:

###### ITEM RESPONSE TIMES (33/2, 74/8)

First, we consider item-by-item and overall survey **response times** in the 'ordinary' `ESM` questionnaires. This information was encoded in the `ESM.RT` dataset in section 2.1. Thus, as a first step, we process those data by cleaning and filtering the same cases cleaned and filtered in the `ESM` dataset.

<details><summary>Show code</summary>
<p>
```{r }
# recoding IDs
# ...................................................

ESM.RT <- participantID_recoding(ESM.RT) 

# recoding time
# ...................................................

# converting temporal variables as POSIXct in the ESM dataset
ESM.RT[,c("RunTimestamp","SubmissionTimestamp")] <- # no need to specify format since they already are POSIXct
  lapply(ESM.RT[,c("RunTimestamp","SubmissionTimestamp")],function(x) as.POSIXct(x,tz="GMT"))

# correcting wrongly encoded RunTimestamps (adding 1year) and SubmissionTimestamps (RunTimestamp + 61.20 sec)
ESM.RT[substr(ESM.RT$RunTimestamp,1,7)=="2018-01","SubmissionTimestamp"] <-
  ESM.RT[substr(ESM.RT$RunTimestamp,1,7)=="2018-01","RunTimestamp"] + 61.20
ESM.RT[substr(ESM.RT$RunTimestamp,1,7)=="2018-01",c("RunTimestamp","SubmissionTimestamp")] <-
  ESM.RT[substr(ESM.RT$RunTimestamp,1,7)=="2018-01",c("RunTimestamp","SubmissionTimestamp")] + 24*60*60*365

# sanity check: min and max matching with the data collection period
c(min(ESM.RT$RunTimestamp),max(ESM.RT$RunTimestamp)) 

# adding 1 hour to all ESM timestamps
ESM.RT[,c("RunTimestamp","SubmissionTimestamp")] <- ESM.RT[,c("RunTimestamp","SubmissionTimestamp")] + 1*60*60

# adding 1 further hour to ESM timestamps during daylight saving time
ESM.RT$daylight <- "standard"
ESM.RT[ESM.RT$RunTimestamp > as.POSIXct("2019-04-01") & ESM.RT$RunTimestamp < as.POSIXct("2019-10-27"),"daylight"] <- "daylight"
ESM.RT[ESM.RT$daylight=="daylight",c("RunTimestamp","SubmissionTimestamp")] <- 
  ESM.RT[ESM.RT$daylight=="daylight",c("RunTimestamp","SubmissionTimestamp")] + 1*60*60

# creating day.of.week
ESM.RT$day.of.week <- as.POSIXlt(ESM.RT$RunTimestamp)$wday - 1

# creating within.day
ESM.RT <- ddply(ESM.RT,c("ID","day.of.week"),transform,within.day=seq_along(day.of.week))
ESM.RT <- within.day.adjust(ESM.RT)

# recoding other variables
# ....................................................

# ID as factor
ESM.RT$ID <- as.factor(ESM.RT$ID)

# Sorting and renaming columns
ESM.RT <- ESM.RT[,c("ID","day.of.week","within.day","RunTimestamp","SubmissionTimestamp","RT.tot",
                    "v1.male.bene","v2.soddisfatto.insoddisfatto","v3.positivo.negativo", # hedonic tone
                    "t1.rilassato.teso","t2.agitato.calmo","t3.nervoso.tranquillo", # calmness
                    "e1.stanco.sveglio","e2.pieno.privodenergia","e3.affaticato.fresco", # energetic arousal
                    "PE","intensity.PE","NE","intensity.PE", # emotional events
                    "smoke","coffe","alcohol","Activity","People.rec")] # confounders
colnames(ESM.RT)[7:ncol(ESM.RT)] <- c("v1","v2","v3","t1","t2","t3","f1","f2","f3", # renaming columns
                                      "PE","PE.int","NE","NE.int","smoke","coffe","alcohol","activity","people")

# filtering double responses
# ..................................................

# by RunTimestamp
ESM.RT$idday <- paste(ESM.RT$ID,ESM.RT$RunTimestamp,sep="_")
cat(nrow(ESM.RT[duplicated(ESM.RT$idday),]),"cases of double responses (same ID and RunTimestamp)") # 10 (= ESM)
ESM.RT <- ESM.RT[!(ESM.RT$idday=="HRV007_2018-11-13 16:18:32.769" & is.na(ESM.RT$activity)),] # excl 2nd resp for HRV007 & HRV073
ESM.RT <- ESM.RT[!(ESM.RT$idday=="HRV073_2019-05-08 13:42:51.806" & is.na(ESM.RT$activity)),]
ESM.RT <- ESM.RT[!duplicated(ESM.RT$idday),] # excluding all remaining double responses

# by day.of.week and within.day
ESM.RT$idday <- paste(ESM.RT$ID,ESM.RT$day.of.week,ESM.RT$within.day,sep="_")
cat(nrow(ESM.RT[duplicated(ESM.RT$idday),]),"cases of double responses (same ID and RunTimestamp)") # 6 (4 less than ESM (?))
ESM.RT <- ESM.RT[!duplicated(ESM.RT$idday),] # excluding all double responses

# marking further excluded
# ..................................................

# marking 8 excluded participants as "OUT"
ESM.RT$ID <- as.character(ESM.RT$ID)
excl <- c("HRV009","HRV011","HRV015","HRV027","HRV029","HRV046","HRV059","HRV066")
ESM.RT[ESM.RT$ID%in%excl,"ID"] <- gsub("HRV","OUT",ESM.RT[ESM.RT$ID%in%excl,"ID"]) # marking excluded participants as "OUTXXX"
ESM.RT$ID <- as.factor(ESM.RT$ID)
ESM.RT <- ESM.RT[order(ESM.RT$ID,ESM.RT$RunTimestamp),]

# excluding cases with missing response to most items
# .................................................

ESM.RT <- ESM.RT[!is.na(ESM.RT$v3),]

# sanity checks
# .................................................

# same number of participants?
nlevels(ESM.RT$ID) == nlevels(ESM$ID)

# same number of included participants?
nlevels(as.factor(as.character(ESM.RT[substr(ESM.RT$ID,1,3)=="HRV","ID"]))) ==
  nlevels(as.factor(as.character(ESM[substr(ESM$ID,1,3)=="HRV","ID"])))

# same number of observations?
nrow(ESM.RT) == nrow(ESM[ESM$within.day!=0,])
```
</p></details>

<br>

Now we can inspect the **item-by-item response times** (seconds) in the `ESM.RT` dataset, computed as the difference between the timestamp of each response and that of the previous one (note: the first response of each questionnaire does not have an associated response time). 

We can see that **responding to a single item required about 2 seconds** in most cases (median response time = 2.69 sec), regardless of the type of items. A small number of items were responded in more than 10 seconds (4.3%) with a very few cases requiring more than 50 seconds (N = 41, 0.18%). Likewise, a small number of items were responded in less than 1 second (2.4%) with a very few cases requiring less than 0.65 seconds (N = 109, 0.48%).
```{r fig.width=12,fig.height=4,warning=FALSE}
# item-by-item long dataset
rt <- melt(ESM.RT[,c("ID","RunTimestamp",colnames(ESM.RT)[which(colnames(ESM.RT)=="v1"):which(colnames(ESM.RT)=="people")])],
           id=c("ID","RunTimestamp"))
rt <- rt[!is.na(rt$value),] # removing missing values
rt$items <- "mood" # categorizing variables based on the item contents
rt[rt$variable%in%c("PE","PE.int","NE","NE.int"),"items"] <- "event"
rt[rt$variable%in%c("smoke","coffe","alcohol","activity","people"),"items"] <- "conf"

# summarizing response times
summary(rt$value)

# plotting response times
grid.arrange(ggplot(rt,aes(value))+geom_histogram(bins=300)+ggtitle("Response times (sec)"),
             ggplot(rt,aes(value))+geom_histogram(bins=300)+xlim(0,100)+ggtitle("zooming 0 - 100 sec"),
             ggplot(rt,aes(value))+geom_histogram(bins=300)+xlim(0,10)+ggtitle("zooming 0 - 10 sec"),
             ggplot(rt[rt$items=="mood",],aes(value))+geom_histogram(bins=300,alpha=0.5)+xlim(0,10) + ggtitle("Mood items"),
             ggplot(rt[rt$items=="event",],aes(value))+geom_histogram(bins=300,alpha=0.5)+xlim(0,10) + ggtitle("Event items"),
             ggplot(rt[rt$items=="conf",],aes(value))+geom_histogram(bins=300,alpha=0.5)+xlim(0,10) + ggtitle("Confounders items"),
             nrow=2)

# No. of response times higher than X
for(value in c(10,20,25,50,100,500,1000,2000,4000)){ cat("\n-",nrow(rt[rt$value>value,]),"RTs >",value,
                                                      "s (",round(100*nrow(rt[rt$value>value,])/nrow(rt),2),"% )") }

# No. of response times lower than X
for(value in c(2,1.5,1,0.65,0.5,0.25,0.1)){ cat("\n-",nrow(rt[rt$value<value,]),"RTs <",value,
                                           "s (",round(100*nrow(rt[rt$value<value,])/nrow(rt),2),"% )") }
```

<br>

Here, we **flag** all cases with at least one item responded in **more than 50 seconds** with `iRT.slow = 1` and all those responded in **less than 650 ms** with `iRT.fast = 1`. While the first threshold is essentially based on the observed RT distribution, the second criterion was also recommended by [Geeraerts (2020)](#ref). We also compute the number of items meeting each condition, for each survey.

We can see that most cases have only one slow-response-time item (N = 26) with only 7 surveys showing 2 or more items that took more than 50 sec. In contrast, there is a similar proportion of surveys with either 1 (N = 39) or 2 items (N = 35) answered with less than 650 ms. In terms of **careless participants**, we flag slow and fast respondents considering participants with 3+ slow responses (N = 2) and those with 3+ fast responses (N = 8 + one excluded participant). Particular attention should be posed on participant `HRV099`, showing 11 out of 14 responses with fast response times.
```{r }
ESM.RT$idday <- as.factor(paste(ESM.RT$ID,ESM.RT$RunTimestamp,sep="_"))
rt$idday <- as.factor(paste(rt$ID,rt$RunTimestamp,sep="_"))
ESM.RT$iRT.slow <- ESM.RT$NiRT.slow <- ESM.RT$iRT.fast <- ESM.RT$NiRT.fast <- ESM.RT$slowResp <- ESM.RT$fastResp <- 0
for(idday in levels(ESM.RT$idday)){
  if(any(rt[rt$idday==idday,"value"]>50)){ ESM.RT[ESM.RT$idday==idday,"iRT.slow"] <- 1
    ESM.RT[ESM.RT$idday==idday,"NiRT.slow"] <- nrow(rt[rt$idday==idday & !is.na(rt$value) & rt$value>50,]) }
  if(any(rt[rt$idday==idday,"value"]<0.65)){ ESM.RT[ESM.RT$idday==idday,"iRT.fast"] <- 1
    ESM.RT[ESM.RT$idday==idday,"NiRT.fast"] <- nrow(rt[rt$idday==idday & !is.na(rt$value) & rt$value<0.65,]) }}

# slow response times (N = 33)
summary(as.factor(ESM.RT$NiRT.slow)) # summary of number of slow response times
(s <- summary(as.factor(as.character(ESM.RT[ESM.RT$iRT.slow==1,"ID"])))) # summary of slow respondents
ESM.RT[ESM.RT$ID %in% names(s[s >= 3]),"slowResp"] <- 1 # flagging slow respondents (3+ responses with 1+ items > 50 sec)
summary(as.factor(ESM.RT[!duplicated(ESM.RT$ID),"slowResp"])) # No. of slow respondents (N = 2)

# fast response times (N = 74)
summary(as.factor(ESM.RT$NiRT.fast)) # summary of number of fast response times
(s <- summary(as.factor(as.character(ESM.RT[ESM.RT$iRT.fast==1,"ID"])))) # summary of fast respondents
ESM.RT[ESM.RT$ID %in% names(s[s >= 3]),"fastResp"] <- 1 # flagging fast respondents (i.e., 3+ responses with 1+ items > 50 sec)
summary(as.factor(ESM.RT[!duplicated(ESM.RT$ID),"fastResp"])) # No. of fast respondents (N = 8 + 1 excluded)
```

<br>

###### TOTAL RESPONSE TIMES (29/1, 27/3)

Second, the same procedures used with item response times are applied considering the **total response time of each survey** `RT.tot`, computed in section 2.1 as the seconds between the first response and the `SubmissionTimestamp` of each survey. Note that here we do not consider those cases with missing response to the last items (i.e., from `PE` on out).

We can see that **responding to the whole questionnaire required about 1 min** in most cases (median = 61.2 sec). A small number of questionnaires were responded in more than 2 minutes (7.3%) with a very few cases requiring more than 3 min (N = 29, 2.13%, flagged as `tRT.slow = 1`). Likewise, a small number of questionnaires were responded in less than 40 second (8.2%) with a very few cases requiring less than 35 seconds (N = 27, 2%, flagged as `tRT.fast = 1`). We also flag **careless participants** as those with 3+ responses that took more than 5 min (N = 1 flagged as `slowResp.tot = 1`) and 3+ responses that took less than 35 seconds (N = 3 flagged as `fastResp.tot = 1`).
```{r fig.width=12,fig.height=2,warning=FALSE}
# summarizing total response times (in both seconds and minutes)
ESM.RT$RT.tot.min <- ESM.RT$RT.tot/60
summary(ESM.RT$RT.tot); summary(ESM.RT$RT.tot.min)

# plotting response times
grid.arrange(ggplot(ESM.RT,aes(RT.tot.min))+geom_histogram(bins=300)+ggtitle("Total response times (min)"),
             ggplot(ESM.RT,aes(RT.tot.min))+geom_histogram(bins=300)+xlim(0,10)+ggtitle("zooming 0 - 10 min"),
             ggplot(ESM.RT,aes(RT.tot.min))+geom_histogram(bins=300)+xlim(0,5)+ggtitle("zooming 0 - 5 min"),
             nrow=1)

# No. of total response times higher than X minutes
for(value in c(2,3,5,10,20,50)){ cat("\n-",nrow(ESM.RT[ESM.RT$RT.tot.min>value,]),"total RTs >",value,
                                                      "min (",round(100*nrow(ESM.RT[ESM.RT$RT.tot.min>value,])/
                                                                      nrow(ESM.RT),2),"% )") }

# No. of total response times lower than X seconds
for(value in c(40,35,30,20)){ cat("\n-",nrow(ESM.RT[!is.na(ESM.RT$PE) & ESM.RT$RT.tot<value,]),"total RTs <",value,
                                                      "sec (",round(100*nrow(ESM.RT[!is.na(ESM.RT$PE) &
                                                                                             ESM.RT$RT.tot<value,])/
                                                                                      nrow(ESM.RT[!is.na(ESM.RT$PE),]),2),"% )") }

# flagging slow > 5 min (N = 29) and fast total response times < 35 sec (N = 27)
ESM.RT$tRT.slow <- ESM.RT$tRT.fast <- ESM.RT$slowResp.tot <- ESM.RT$fastResp.tot <- 0
ESM.RT[ESM.RT$RT.tot.min > 3,"tRT.slow"] <- 1
ESM.RT[!is.na(ESM.RT$PE) & ESM.RT$RT.tot < 35,"tRT.fast"] <- 1

# flagging slow and fast respondents
(s <- summary(as.factor(as.character(ESM.RT[ESM.RT$tRT.slow==1,"ID"])))) # summary of slow respondents
ESM.RT[ESM.RT$ID %in% names(s[s >= 3]),"slowResp.tot"] <- 1 # flagging slow respondents (3+ responses > 5 min)
summary(as.factor(ESM.RT[!duplicated(ESM.RT$ID),"slowResp.tot"])) # No. of slow respondents (N = 1)
(s <- summary(as.factor(as.character(ESM.RT[ESM.RT$tRT.fast==1,"ID"])))) # summary of fast respondents
ESM.RT[ESM.RT$ID %in% names(s[s >= 3]),"fastResp.tot"] <- 1 # flagging fast respondents (3+ responses < 35 sec)
summary(as.factor(ESM.RT[!duplicated(ESM.RT$ID),"fastResp.tot"])) # No. of slow respondents (N = 3)
```

<br>

###### LONG STRING PROPORTION (57/4)

Third, we examine those cases with extreme **long string of identical responses** to mood items, that is extremely consistent responses with long strings of identical value in consecutive items using the same response scale (see [Curran (2016)](#ref)). This is done using the `careless` package. Note that in section 2.2.3 we recoded mood ratings to have the same polarity. Event items are not considered because most respondents entered no events (i.e., 1, 1).

We can see that in most cases (about 71%) the same rating was entered for up to 3 consecutive items, with the 14% showing strings of consistent responses repeated more than 4 times (i.e., the rule of thumb proposed by [Curran (2016)](#ref) of identical string length > scale range / 2). In a minority of cases (N = 57, 3.4%), respondents entered the same value for 7+ times, and we mark them as `ls = 1`. As done previously, we also flag **careless participants** (N = 4) as those with 3+ responses with long identical strings (N = 18 flagged as `lsResp = 1`).
```{r fig.width=12,fig.height=2,warning=FALSE}
# computing the longest string frequency and the average length of identical consecutive responses
mood <- colnames(ESM)[which(colnames(ESM)=="v1"):which(colnames(ESM)=="f3")]
ESM.LS <- cbind(ESM[,c("ID","RunTimestamp","within.day")],longstring(ESM[,mood],avg=TRUE))

# plotting longest identical string and average identical string length
grid.arrange(ggplot(ESM.LS,aes(longstr))+geom_histogram(bins=30)+ggtitle("Longest identical string in each survey"),
             ggplot(ESM.LS,aes(avgstr))+geom_histogram(bins=30)+ggtitle("Average identical string length"),
             nrow=1)

# No. of longest identical strings longer than X
for(value in 3:6){ cat("\n-",nrow(ESM.LS[ESM.LS$longstr>value,]),"longest identical string >",value,
                       "times (",round(100*nrow(ESM.LS[ESM.LS$longstr>value,])/nrow(ESM.LS),2),"% )") }

# flagging cases with identical strings > 6 (N = 57)
ESM.LS$lsResp <- ESM.LS$ls <- 0
ESM.LS[ESM.LS$longstr > 6,"ls"] <- 1
summary(as.factor(ESM.LS$ls)) # No. of cases with long string (N = 57)

# flagging long string respondents (N = 4)
(s <- summary(as.factor(as.character(ESM.LS[ESM.LS$ls == 1,"ID"])))) # No. of long string responses per respondent
ESM.LS[ESM.LS$ID %in% names(s[s >= 3]),"lsResp"] <- 1 # flagging slow respondents (i.e., 2+ identical strings > 5)
summary(as.factor(ESM.LS[!duplicated(ESM.LS$ID),"lsResp"]))
```

<br>

###### MAHALANOBIS DISTANCE (72/6)

Fourth, we examine those cases with **unlikely response patterns** by performing a **multivariate outlier analysis** based on the Mahalanobis distance. We can see that 72 responses (4.3%) were flagged as multivariate outliers (`md = 1`) with a confidence level of 99%, with 6 participants showing 3+ flagged responses and thus marked with `mdResp = 1`.
```{r fig.width=6,fig.height=4,warning=FALSE}
# computing Mahalanobis distance in mood ratings
ESM.LS <- cbind(ESM.LS,mahad(ESM[,mood],flag=TRUE))

# plotting Mahalanobis distance and marking flagged cases (N = 72)
ESM.LS$md <- as.numeric(gsub("FALSE","0",gsub("TRUE","1",ESM.LS$flagged)))
summary(as.factor(ESM.LS$md)) # No. flagged responses (N = 72)
ggplot(ESM.LS,aes(d_sq,fill=as.factor(md)))+geom_histogram(bins=30)+ggtitle("Mahalanobis distance")

# flagging outlier respondents (N = 6)
(s <- summary(as.factor(as.character(ESM.LS[ESM.LS$md == 1,"ID"])))) # No. of flagged responses per respondent
ESM.LS$mdResp <- 0
ESM.LS[ESM.LS$ID %in% names(s[s >= 3]),"mdResp"] <- 1 # flagging outlier respondents (i.e., 3+ extreme Mahalanobis dist)
summary(as.factor(ESM.LS[!duplicated(ESM.LS$ID),"mdResp"])) # No. flagged respondents (N = 6)
```

<br>

###### PSYCHOMETRIC SYNONIMS (90/4)

Fifth, we examine those cases with **negative correlations among psychometric synonyms**, that is cases where the intra-response correlation is negative, and specifically less than -0.30 between pairs of items that correlate at *r* = .60 or stronger. We can see that 87 responses (5.3%) were flagged as `psychsyn = 1`, with 5 participants showing 3+ flagged responses and thus marked with `psychsynResp = 1`.
```{r fig.width=7,fig.height=3,warning=FALSE}
# computing Mahalanobis distance in mood ratings
ESM.LS <- cbind(ESM.LS,psychsyn(ESM[,mood],diag=TRUE,critval=0.60,resample_na=TRUE))

# note that some cases marked with long identical string have SD = 0 so the intra-response correlation cannot be computed
head(cbind(ESM.LS[is.na(ESM.LS$cor),c("ID","longstr","avgstr","ls","lsResp")],ESM[is.na(ESM.LS$cor),mood]))

# flagging and plotting correlations < -.30 (N = 90)
ESM.LS$psychsyn <- 0
ESM.LS[!is.na(ESM.LS$cor) & ESM.LS$cor < (-.30),"psychsyn"] <- 1
summary(as.factor(ESM.LS$psychsyn)) # No flagged responses (N = 90)
ggplot(ESM.LS,aes(cor,fill=as.factor(psychsyn)))+geom_histogram(bins=100)+ggtitle("Indivual corr. among psychometric synonyms")

# flagging psychsyn respondents (N = 5)
(s <- summary(as.factor(as.character(ESM.LS[ESM.LS$psychsyn == 1,"ID"])))) # No. of flagged responses per respondent
ESM.LS$psychsynResp <- 0
ESM.LS[ESM.LS$ID %in% names(s[s >= 3]),"psychsynResp"] <- 1 # flagging outlier respondents (i.e., 3+ flagged responses)
summary(as.factor(ESM.LS[!duplicated(ESM.LS$ID),"psychsynResp"])) # No. flagged participants (N = 4 + 1 excluded)
```

<br>

##### Summary of careless responses

Here, we summarize the number of potentially careless responses and participants for each criterion, and we compute an aggregated variable indicating the number of criteria (from 1 to 5) based on which each response and participant is classified as potentially careless. Then, we flag all **responses** classified as potentially careless based on 1+ criteria (N = 317, of which 293 from included participants) with `cLess1 = 1` and those based on 2+ criteria (N = 57, of which 51 from included participants) with `cLess2 = 1`, respectively. Similarly, we flag all **participants** classified as potentially careless based on 1+ criteria (N = 26, of which 23 included participants) with `cLessResp1 = 1`, and those based on 2+ criteria (N = 4, of which 3 included participants) with `cLessResp2 = 1`.
```{r fig.width=6,fig.height=2,warning=FALSE}
# joining careless criteria
ESM.LS$idday <- as.factor(paste(ESM.LS$ID,ESM.LS$RunTimestamp,sep="_"))
ESM.careless <- join(ESM.LS[,c("ID","within.day","idday","ls","md","psychsyn","lsResp","mdResp","psychsynResp")],
                     ESM.RT[,c("idday","iRT.fast","tRT.fast","iRT.slow","tRT.slow",
                               "fastResp","fastResp.tot","slowResp","slowResp.tot")],by="idday",type="left")

# computing aggregate careless scores
ESM.careless$cLessResp2 <- ESM.careless$cLessResp1 <- ESM.careless$cLess2 <- ESM.careless$cLess1 <- 0
cr <- c("ls","md","psychsyn","iRT.fast","tRT.fast","iRT.slow","tRT.slow")
ESM.careless$carelessN <- rowSums(ESM.careless[,cr],na.rm=TRUE)
ESM.careless[ESM.careless$carelessN>=1,"cLess1"] <- 1 # careless response if meeting 1+ criteria
ESM.careless[ESM.careless$carelessN>=2,"cLess2"] <- 1 # careless response if meeting 2+ criteria
crResp <- c("lsResp","mdResp","psychsynResp","fastResp","fastResp.tot","slowResp","slowResp.tot")
ESM.careless$cLessRespN <- rowSums(ESM.careless[,crResp],na.rm=TRUE)
ESM.clResp <- ESM.careless[ESM.careless$within.day!=0,]
ESM.clResp <- ESM.clResp[!duplicated(ESM.clResp$ID),]
ESM.clResp[ESM.clResp$cLessRespN>=1,"cLessResp1"] <- 1 # careless respondent if meeting 1+ criteria
ESM.clResp[ESM.clResp$cLessRespN>=2,"cLessResp2"] <- 1 # careless respondent if meeting 1+ criteria

# summarizing criteria of potentially careless responses (N = 51)
summary(as.data.frame(lapply(ESM.careless[,c(cr,"carelessN","cLess1","cLess2")],as.factor))) # all in
summary(as.data.frame(lapply(ESM.careless[substr(ESM.careless$ID,1,3)!="OUT",
                                          c(cr,"carelessN","cLess1","cLess2")],as.factor))) # only included participants

# summarizing criteria of potentially careless respondents (N = 16)
summary(as.data.frame(lapply(ESM.clResp[,c(crResp,"cLessRespN","cLessResp1","cLessResp2")],as.factor))) # all in
summary(as.data.frame(lapply(ESM.clResp[substr(ESM.clResp$ID,1,3)!="OUT",
                                        c(crResp,"cLessRespN","cLessResp1","cLessResp2")],as.factor))) # only included

# joining careless scores to the ESM dataset
ESM$idday <- as.factor(paste(ESM$ID,ESM$RunTimestamp,sep="_"))
ESM <- join(ESM,ESM.careless[,c("idday","cLess1","cLess2")],by="idday",type="left")
ESM <- join(ESM,ESM.clResp[,c("ID","cLessResp1","cLessResp2")],by="ID",type="left")
```

<br>

Here, we visually inspect all responses marked with `cLess2 = 1`. We can note several cases of long string proportions (e.g., `HRV013_2_3`) as well as cases of inconsistent responses (e.g., `HRV060_1_0` for items `t2` and `t3`).
```{r fig.width=8,fig.height=10}
ESM$id <- as.factor(paste(ESM$ID,ESM$day.of.week,ESM$within.day,sep="_"))
plotResp(ESM[ESM$cLess2==1,],"id",mood)
```

<br>

Here, we visually inspect all responses from participants marked with `cLessResp2 = 2`. Also in this case, we can note several cases of long string proportions (e.g., `HRV051_2_4`) and especially cases of inconsistent responses (e.g., `HRV051_1_1` for items `f2` and `f3`).
```{r fig.width=8,fig.height=10}
plotResp(ESM[ESM$cLessResp2==1,],"id",mood)
```

<br>

# 3. HRV data

ARRIVATO QUI

# 4. GNG data

# 5. Data merging

# 6. Data dictionary

# 7. Data export

<br>

NOTE PER LE ANALISI:
nel valutare la distribuzione da usare:
- ex-Gaussian, quasi normale con coda positiva (positively skewed), ma mu e sigma sono indipendenti
- Gamma: come sopra ma mu e sigma sono correlate -> per vederlo conviene plottare RMSSD per diverse categorie di X
   e vedi se la variabilità aumenta con la media dell'RMSSD
- nota: però la Gamma inizia a zero!

# References {#ref}

- Curran, P. G. (2016). Methods for the detection of carelessly invalid responses in survey data. Journal of Experimental Social Psychology, 66, 4-19. https://doi.org/10.1016/j.jesp.2015.07.006

- Geeraerts, J. (2020, May 20). Investigating careless responding detection techniques in experience sampling methods. Retrieved from https://osf.io/d798j/

- Xiong, H., Huang, Y., Barnes, L. E., & Gerber, M. S. (2016). Sensus: a cross-platform, general-purpose system for mobile crowdsensing in human-subject studies. *Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing*, 415–426. https://doi.org/10.1145/2971648.2971711

## R packages
