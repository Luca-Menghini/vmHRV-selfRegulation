---
title: "Inter- and intraindividual relationships between vagally-mediated heart rate variability and self-regulatory processes: An ecological momentary assessment"
subtitle: "Supplementary material 2: Data pre-processing"
author: "Luca Menghini, MS$^1$, Giulia Fuochi, PhD$^2$, Michela Sarlo, PhD$^3$"
date:  "`r Sys.Date()`"
bibliography: [packagesProc.bib]
nocite: '@*'
output: 
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    toc_float: true
    css: styles.css
---

<br>

$^1$Department of General Psychology, University of Padova, Italy

$^2$Department of Philosophy, Sociology, Pedagogy and Applied Psychology, University of Padova, Italy

$^3$Department of Communication Sciences, Humanities and International Studies, University of Urbino Carlo Bo, Italy

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

# Aims and content

The present document includes the data pre-processing steps used to read the different types of data collected over three days from a sample of 105 healthy adults:

- `PrelQS` = demographic & retrospective self-report data collected with the preliminary questionnaire ([Google Forms](https://www.google.com/intl/it/forms/about/))

- `ESM` = experience sampling measures collected with the [Sensus Mobile app](https://predictive-technology-laboratory.github.io/sensus/) ([Xiong et al., 2016](#ref))

- `HRV` = 2-min segments of blood volume pulse data recorded with the E4 wristband (Empatica, Milan)

- `GNG` = Go/No-Go behavioral data recorded with E-Prime 2.0.10 (Psychology Software Tools, Inc., Sharpsburg, PA) 

<br>

For each data type, the following pre-processing steps are implemented to generate the `wide` and `long` datasets to be used for subsequent data analysis (see analytical reports [1. Psychometrics and descriptives](https://luca-menghini/vmHRV-selfRegulation/) and [2. Regression models](https://luca-menghini/vmHRV-selfRegulation/)):

1. Raw data files **reading & merging** 

2. Raw data **recoding & processing**

3. Data **filtering** based on inclusion criteria and data quality

<br>

Here, we remove all objects from the R global environment, and we set the system time zone for better temporal synchronization across data files.
```{r warning=FALSE}
# removing all objets from the workspace
rm(list=ls())

# setting system time zone to GMT (for consistent temporal synchronization)
Sys.setenv(tz="GMT")
```

<br>

The following R packages are used in this document (see [References section](#ref)):
```{r }
# required packages
packages <- c("ggplot2","gridExtra","jsonlite","plyr","lubridate")

# generate packages references
knitr::write_bib(c(.packages(), packages),"packagesProc.bib")

# # run to install missing packages
# xfun::pkg_attach2(packages, message = FALSE); rm(list=ls())
```

<br>
<br>

# 1. PrelQS data

Here, the retrospective self-report data collected with the **preliminary questionnaire** are read and saved in the `PrelQS` dataset, recoded, and filtered based on the inclusion criteria.
```{r warning=FALSE,message=FALSE}
library(ggplot2);library(gridExtra) # loading required packages
```

<br>

## 1.1. Data reading

First, we read the `Preliminary_qs.csv` data file exported from [Google Forms](https://www.google.com/intl/it/forms/about/).
```{r }
PrelQS <- read.csv("qs preliminare/Preliminary_qs.csv")
```

<br>

## 1.2. Data recoding

Then, we recode the `PrelQS` variables to be used for the analyses.

### 1.2.1. ID recoding

As a first step, we **recode participants' identification codes** `ID` (currently corresponding to their e-mail addresses) using the "HRVXXX" format (e.g., from '[john.smith\@gmail.com](mailto:john.smith@gmail.com){.email}' to 'HRV001'). This is done with the `prel.qs_IDrecode()` function. Since the participants' e-mail addresses are confidential, both the function and the original dataset are not showed. 
```{r }
# renaming the first two variables
colnames(PrelQS)[1:2] <- c("timestamp","ID") 

# loading and using the function to recode the ID variable
source("prel.qs_IDrecode.R") 
(PrelQS <- prel.qs_IDrecode(PrelQS))[1:3,] # showing first three rows
```

<br>

### 1.2.2. Other variables

Second, we rename the other variables, remove those not considered for the present study, and recode the considered variables to be used in the analyses.
```{r }
# renaming all variables
colnames(PrelQS) <- c("time","ID", # submission timestamp & participants ID
                      "consent", # consent to participate (all 'yes')
                      "age","sex","weight","height","itaMT", # demographics
                      
                      # inclusion criteria
                      "drugs","drugs.which","cvDysf","cvDysf.which",
                      "otherDysf","otherDysf.which","phone","phone.which",
                      
                      # retrospective scales (* = not considered in this study)
                      paste("DASS21",1:21,sep=""), # Depression, Anxiety, and Stress Scale*
                      paste("PANAS",1:20,sep=""), # Positive and Negative Affective Schedule*
                      paste("DERS",1:36,sep=""), # Difficulties in Emotion Regulation Scale*
                      paste("BIS11",c(1:15,17:20,22,23,25:30),sep=""), # Barratt Impulsiveness Scale-11 (3 items out)
                      paste("PSI",1:17,sep="")) # Physical Symptoms Inventory*

# keeping only considered variables (demographics and inclusion criteria)
PrelQS <- PrelQS[,c("ID","time",colnames(PrelQS)[which(colnames(PrelQS)=="age"):which(colnames(PrelQS)=="height")],
                    colnames(PrelQS)[which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")])]

# recoding variables
PrelQS[,c("ID","sex")] <- lapply(PrelQS[,c("ID","sex")],as.factor) # ID and sex as factor
PrelQS$time <- as.POSIXct(PrelQS$time,format="%Y/%m/%d %I:%M:%S %p") # time as POSIXct
PrelQS[PrelQS$height<3,"height"] <- PrelQS[PrelQS$height<3,"height"]*100 # correcting heights reported in meters
PrelQS <- cbind(PrelQS[,1:4],BMI=PrelQS$weight/(PrelQS$height/100)^2, # computing BMI, removing weight & height
                PrelQS[,7:ncol(PrelQS)])
for(i in which(colnames(PrelQS)%in%c("drugs","cvDysf","otherDysf","phone"))){ PrelQS[,i] <- gsub("Sì","Yes",PrelQS[,i]) }
PrelQS[,which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")] <- # inclusion criteria as factors
  lapply(PrelQS[,which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")],as.factor)

# sorting dataset by ID and time
PrelQS <- PrelQS[order(PrelQS$ID,PrelQS$time),]
```

<br>

### 1.2.3. Data structure

Here, we display the structure of the `PrelQS` processed dataset.
```{r }
str(PrelQS)
```

<br>

## 1.3. Cleaning & filtering

Here, we inspect the original number of cases in the `PrelQS` dataset. The questionnaire was completed by a total of 164 respondents, of which a subsample was invited to participate in the daily protocol. Here, we clean and filter the data based on the inclusion criteria, and considering the actual sample of 105 participants.
```{r }
cat("Original No. of responses to the PrelQS =",nrow(PrelQS))
```

<br>

### 1.3.1. Data cleaning

First, we inspect and remove cases of **double responses** (N = 3 couples of responses with the same `ID` value). In these cases, only the first response (i.e., the one with the earilest timestamp) is included.
```{r }
# detecting double responses
cat(nrow(PrelQS[duplicated(PrelQS$ID),]),"cases of double responses (same ID)")

# removing double responses
memory <- PrelQS
PrelQS <- PrelQS[!duplicated(PrelQS$ID),]
cat("Removed",nrow(memory)-nrow(PrelQS),"double responses")
```

<br>

### 1.3.2. Inclusion criteria {.tabset .tabset-fade .tabset-pills}

Second, we take a look at the variables concerning the **inclusion criteria** of the study:

1. Not suffering from **dysfunctions** (e.g., anxiety disorder) or taking **medications** affecting the *nervous system* (e.g., antidepressants)

2. Not suffering from **dysfunctions** (e.g., hypertension) or taking **medications** affecting the *cardiovascular system* (e.g., beta blockers)

3. Owning an Android or iOS **smartphone**

<br>

In section 1.2.1., we recoded participants identification codes and we marked the cases of participants not meeting the inclusion criteria, as well as other cases of participants that were not invited to take part in the EMA protocol for other reasons, using the "***OUT***" label in the `ID` variable. From a total of 161 responses to the preliminary questionnaire, we can see that 105 participants (65%) were actually invited to participate in the daily protocol.
```{r }
cat("Total No. of responses to the PrelQS =",nrow(PrelQS),"\n -",
    nrow(PrelQS[substr(PrelQS$ID,1,3)!="OUT",]),"invited\n -",nrow(PrelQS[substr(PrelQS$ID,1,3)=="OUT",]),"not invited")
```

<br>

Here, we better evaluate the reasons for the exclusion of the remaining 56 participants.

#### SUMMARY

- **9** participants were not invited because they reported suffering from **cardiovascular** (i.e., premature heart beat, problems with the mitral valve) or **other dysfunction** (i.e., thalassemia, anxiety, Crohn’s disease) and/or taking **antidepressants**

- **1** participant was not invited because she did not own a smartphone

- Further **46** participants were not invited to join the EMA protocol for other reasons (e.g., calendar incompatibilities, end of the data collection)

<br>

Here, we compare the demographic and retrospective variables between included and excluded participants. Most participants that were not invited to take part in the daily protocol were **female** (probably due to the higher No. of female compared to undergraduates participating to psychological studies, and our goal of having a balanced sample), with no marked differences in terms of `sex`, `age`, `BMI`
```{r fig.width=10,fig.height=2}
# preparing data
PrelQS$excl <- "IN" # creating excl variable (factor)
PrelQS[substr(PrelQS$ID,1,3)=="OUT","excl"] <- "OUT"
PrelQS$excl <- as.factor(PrelQS$excl)

# plotting
grid.arrange(ggplot(PrelQS,aes(x=excl,fill=sex)) + geom_bar(),
             ggplot(PrelQS,aes(x=excl,y=age,fill=excl)) + geom_violin(),
             ggplot(PrelQS,aes(x=excl,y=BMI,fill=excl)) + geom_violin(),nrow=1) 

# removing excl
PrelQS$excl <- NULL
```

<br>

#### DYSFUNCTIONS (7)

Here, we show the No. of participants not invited to take part in the EMA protocol due to **dysfunctions** affecting either the *cardiovascular* or the *nervous* system. We can see that 4 females were excluded since they reported suffering from a **cardiovascular dysfunction** including premature heart beat (`027`, `055`) and problems with the mitral valve (`033`, `053`). 3 further females (4.61%) were excluded since they reported suffering from other dysfunctions affecting the **nervous system**, including thalassemia (`009`), anxiety (`020`), Crohn’s disease (`049`).
```{r }
# inclusion criteria variables
ic <- colnames(PrelQS)[which(colnames(PrelQS)=="drugs"):which(colnames(PrelQS)=="phone.which")]

# cardiovascular dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$cvDysf=="Yes", c("ID","sex",ic)]

# other dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$otherDysf=="Yes", c("ID","sex",ic)]
```

<br>

#### MEDICATIONS (3)

Here, we show the No. of participants not invited to take part in the EMA protocol due to **medications** affecting either the *cardiovascular* or the *nervous* system. 3 participants (`017`, F; `029`, M; `033`, M) were excluded since they took **antidepressants** (Laroxyl, Venlafaxina, Remeron), one of which also suffered from a cardiovascular dysfunction.
```{r }
# cardiovascular dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$drugs=="Yes", c("ID","sex",ic)]
```

<br>

#### SMARTPHONE (1)

Here, we show the No. of participants not invited to take part in the EMA protocol due to **medications** affecting either the *cardiovascular* or the *nervous* system. Only 1 female (`051`) was excluded since she reported to not own a personal smarpthone.
```{r }
# cardiovascular dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & PrelQS$phone=="No", c("ID","sex",ic)]
```

<br>

#### OTHER REASONS (46)

The further 46 participants marked with `"OUT"` were not invited due to other reasons (e.g., calendar incompatibility, end of the study).
```{r }
PrelQS[substr(PrelQS$ID,1,3)=="OUT" & !(PrelQS$ID%in%c("OUT033","OUT053","OUT027","OUT055","OUT009","OUT020",
                                                       "OUT049","OUT017","OUT029","OUT051")),]
```

<br>

### 1.3.3. Flagged participants

Some of the **included participants reported suffering from dysfunctions or taking medications** that were considered irrelevant for the current study. Here, we better inspect those conditions (N = 9 and 13, respectively):

- **6** included participants (4 females, 2 males) reported suffering from **cardiovascular dysfunctions**: arrhythmia, tachycardia episodes, and bicuspid aortic valve

- **3** included participants (2 females, 1 male) reported suffering from **other dysfunctions**: hypothyroidism, asthma, and allergy

- **6** females reported taking **hormonal contraceptives**

- **8** participants took other medications including antihistamines (2 males, 5 females), and thyroid hormones (EUTIROX, 1 female)

```{r }
# cardiovascular dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="HRV" & PrelQS$cvDysf=="Yes", c("ID","sex",ic)]

# other dysfunctions
PrelQS[substr(PrelQS$ID,1,3)=="HRV" & PrelQS$otherDysf=="Yes", c("ID","sex",ic)]

# drugs
PrelQS[substr(PrelQS$ID,1,3)=="HRV" & PrelQS$drugs=="Yes", c("ID","sex",ic)]
```

<br>

Although we considered such conditions as irrelevant for the present study, some of them (especially cardiovascular and other dysfunctions) might play a role. Thus, we create the `dysfun` variable to flag cases of **included participants with cardiovascular and/or other dysfunctions**.
```{r }
# recoding dysfun variable (accounting for both cardiovascular and other dysfunctions)
PrelQS$dysfun <- 0
PrelQS[PrelQS$cvDysf=="Yes" | PrelQS$otherDysf=="Yes","dysfun"] <- 1

# recoding drugs variable
PrelQS$drugs <- gsub("Yes","1",gsub("No","0",PrelQS$drugs))

# removing unnecessary variables
PrelQS[,c("cvDysf","otherDysf","cvDysf.which","otherDysf.which","drugs.which","phone","phone.which")] <- NULL

# summary of dysfun and drugs
PrelQS[,c("dysfun","drugs")] <- lapply(PrelQS[,c("dysfun","drugs")],as.factor) # both as factor
summary(PrelQS[substr(PrelQS$ID,1,3)=="HRV",c("dysfun","drugs")])
```

<br>

### 1.3.4. Further excluded

**8 further participants were excluded** due to dropping-out during the EMA protocol (`009`, F; `059`, M), poor quality of physiological data (`015`, F; `046`, F; `066`, M) or technical problems with the mobile app (`011`, M; `027`, M; `029`, M). Here, we mark these participants as 'OUT'. 
```{r }
# marking 8 excluded participants as "OUT"
memory <- PrelQS
PrelQS$ID <- as.character(PrelQS$ID)
PrelQS[PrelQS$ID=="HRV009" | PrelQS$ID=="HRV011" | PrelQS$ID=="HRV015" | PrelQS$ID=="HRV027" |
         PrelQS$ID=="HRV029" | PrelQS$ID=="HRV046" | PrelQS$ID=="HRV059" |
         PrelQS$ID=="HRV066","ID"] <- paste("OUT0",57:(56+8),sep="") # marking excluded participants as "OUTXXX"
PrelQS$ID <- as.factor(PrelQS$ID)
PrelQS <- PrelQS[order(PrelQS$ID),]

# updating number of included participants
cat(nrow(PrelQS[substr(PrelQS$ID,1,3)=="HRV",]),"included participants out of",
    nrow(memory[substr(memory$ID,1,3)=="HRV",]),"invited participants")
```

<br>
<br>

# 2. ESM data

Here, the raw data collected with the **experience sampling method (ESM)** are read and saved in the `ESM` dataset, recoded, and filtered based on response rate and inclusion criteria.

## 2.1. Data reading

First, we read the JSON data files exported from the [Sensus Mobile app](https://predictive-technology-laboratory.github.io/sensus/) ([Xiong et al., 2016](#ref)). 

Specifically, the [Protocols created with Sensus](https://github.com/Luca-Menghini/vmHRV-selfRegulation/tree/main/1ESMmeasures/sensus/protocols) were configured to store the recorded .JSON data files in our private [AWS S3](https://aws.amazon.com/s3/) bucket, from which they were downloaded and stored in the `ESM/data` folder. From the Sensus app, we also exported the Probe Definition files, which we use to replace the *input IDs* (i.e., by default, each item is associated with an alphanumeic code) with the corresponding *input names* (i.e., the item labels that we set in the protocols). These files were saved in the `ESM/probe` folder.

The `readSurveyData` function is used to optimize the ESM data reading.

<details><summary>`readSurveyData`</summary>
<p>
```{r }
#' @title Read JSON data exported from the Sensus mobile app
#' @param data.path = character string indicating the full path to the folder where the JSON raw data are stored.
#' @param probe.definition = character string indicating the full path to the folder where Probe Definition JSON file(s) (from the Sensus mobile app: Protocol > Probe > Scripted Interaction > Share definition).
#' @param messages = logical value indicating whether the processing steps should be printed (default: TRUE).
readSurveyData <- function(data.path,probe.definition,messages=TRUE){ require(jsonlite)

  # 1. Reading data
  # .......................................
  options(digits.secs=3) # setting No. of digits
  paths = list.files(data.path,recursive=TRUE,full.names=TRUE,include.dirs=FALSE) # listing files in data path
  if(messages==TRUE){ cat("\n\nReading",length(paths),"data files from",data.path,"...")}
  var.names <- c("ParticipantId","Timestamp","InputId","Response","RunTimestamp","SubmissionTimestamp",
                 "ScriptName","ProtocolId","$type") # selecting variables of interest
  data <- as.data.frame(matrix(nrow=0,ncol=9)) # empty dataframe creation and population
  colnames(data) <- var.names
  for(path in paths){ 
    if(file.info(path)$size>0){ # only reading Datum files (i.e., containing ScriptDatum, which is sized > 0 Kb)
      new.data <- read_json(path,simplifyDataFrame=TRUE)
      if(class(new.data)=="data.frame" & !is.null(new.data$Response)){ # only keeping files with Response data
        if(class(new.data$Response)=="data.frame"){ # sometimes the Response column is read as dataframe
          new.data$Response <- as.character(new.data$Response$`$values`)}
        data <- rbind(data,new.data[var.names]) }}} 
  data <- data[!is.na(data$ParticipantId),] # only keeping data with ParticipantId information (i.e., participant identification)
  data <- data[!is.na(data$InputId),] # only keeping data with InputId information (i.e., item identification)
  names(data)[9] <- "os" # $type as OS (android or iOS) # mobile OS information
  data[,9] <- gsub("Sensus.Probes.User.Scripts.ScriptDatum, Sensus","",data[,9]) # removing unuseful information
  
  # 2. From Response Ids to Item labels (based on Probe Definition) 
  # ...............................................................
  if(!is.na(probe.definition)){ if(messages==TRUE){ cat("\n\nConverting ResponseId into InputId based on Probe Definition...")}
    # function to read Probe Definition files
    readProbe <- function(path){ probedefinition <- read_json(path,simplifyDataFrame=TRUE) # first probe definition file
      # reading input labels of the first inputGroup
      inputs <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[1]]$Inputs$`$values`[[1]]$Name
      infos <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[1]] # input labels
      PROTOCOL <- data.frame(protocolName=probedefinition$Protocol$Name,protocolId=probedefinition$Protocol$Id, # protocol name
                             scriptName=infos$Name,inputName=inputs,inputId=infos$Inputs$`$values`[[1]]$Id) # script name
      if(length(probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`)>1){ # reading the following inputGroups
        for(i in 2:length(probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`)){
          inputs <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[i]]$Inputs$`$values`[[1]]$Name
          infos <- probedefinition$ScriptRunners$`$values`$Script$InputGroups$`$values`[[i]]
          PROTOCOL <- rbind(PROTOCOL,data.frame(protocolName=probedefinition$Protocol$Name,
                                                protocolId=probedefinition$Protocol$Id,
                                                scriptName=infos$Name,inputName=inputs,
                                                inputId=infos$Inputs$`$values`[[1]]$Id)) }}
      return(PROTOCOL) }
    paths = list.files(probe.definition,recursive=TRUE,full.names=TRUE,include.dirs=FALSE) # listing files in probe.definition path
    PROTOCOL <- readProbe(paths[1]) # reading the first Probe Definition file
    if(length(list.files(probe.definition))>1){ # reading the following Probe Definition files when > 1
      for(path in paths[2:length(paths)]){ PROTOCOL <- rbind(PROTOCOL,readProbe(path)) }}
    for(i in 1:nrow(data)){ 
      for(j in 1:nrow(PROTOCOL)){ if(!is.na(data[i,3]) & data[i,3]==PROTOCOL[j,5]){ data[i,3] <- as.character(PROTOCOL[j,4]) }}}}
  
  # 3. Cleaning and unlisting Response data
  # ...............................................................
  if(messages==TRUE){ cat("\n\nUnlisting Response data and removing system information...")}
  data$Response <- gsub("list","",data$Response) # cleaning categorical items from Sensus system info
  data$Response <- gsub(paste("c","\\(|\\)",sep=""),"",data$Response)
  data$Response <- gsub("\\(|\\)","",data$Response)
  data$Response <- gsub("\\[|\\]","",data$Response)
  data$Response <- gsub("\\$type` = \"System.Collections.Generic.List`1System.Object, mscorlib, mscorlib\", ","",data$Response)
  data$Response <- gsub("\\$values","",data$Response)
  data$Response <- gsub('``` = ', "",data$Response)
  data$Response <- gsub('\ ', "",data$Response)
  data$Response <- gsub('\"', "",data$Response)
  data$Response <- gsub('\"No\"', "No",data$Response)
  data$Response <- gsub('\"Sì\"', "Si",data$Response)
  if(class(data$Response)=="data.frame"){ data$Response <- as.character(data$Response$`$values`[[1]]) # unlisting Response column
    } else { data$Response <- as.character(data$Response) }
  
  # 4. Encoding time information
  # ...............................................................
  if(messages==TRUE){ cat("\n\nConverting Timestamp variables as POSIXct...")}
  timestamps <- c("Timestamp","RunTimestamp","SubmissionTimestamp") # timestamps variables
  data[,timestamps] <- lapply(data[,timestamps],function(x) as.POSIXct(x,format="%Y-%m-%dT%H:%M:%OS",tz="GMT"))
  
  # 5. Cleaning incomplete double responses
  # ...............................................................
  # in some cases, multiple responses are recorded with the same ParticipantId, RunTimeStamp, and InputId
  if(messages==TRUE){ cat("\n\nLooking for double responses...\n")}
  data$id.r <- paste(data$ParticipantId,data$RunTimestamp,data$InputId,sep="_") # ID-time-item identifier
  data$id.t <- paste(data$ParticipantId,data$RunTimestamp,sep="_") # ID-time identifier
  doubles <- levels(as.factor(data[duplicated(data$id.r),"id.t"])) # double responses (same ID-time-item identifier)
  ndoubles <- rep(length(doubles),2) # No. of double responses
  for(double in doubles){ subId <- levels(as.factor(data[data$id.t==double,"SubmissionTimestamp"])) # submission time
    if(length(subId)>1){ ndoubles[1] <- ndoubles[1] - 1
      maxDouble <- max(as.POSIXct(subId)) # when the SubmissionTimestamp is different -> splitting responses
      data[data$id.t==double,"RunTimestamp"] <- maxDouble - 4*60 # 2nd RunTimeStamp = Submission - 4 min (average response time) 
    } else { ndoubles[2] <- ndoubles[2] - 1
      data <- rbind(data[data$id.t!=double,], # when even the SubmissionTimestamp is different -> removing double resp.
                    data[data$id.t==double & !duplicated(data$id.r),]) }}
  cat(" - Removed",ndoubles[1],"double responses (same ID, InputName, RunTimestamp, & SubmissionTimestamp)\n",
      "- Splitted",ndoubles[2],"double responses with different SubmissionTimestamp\n",
      "  (in these cases, RunTimestamp is recomputed as SubmissionTimestamp - 4 min (average response length)")
  
  # 6. Sorting columns and Reshaping
  # ...............................................................
  if(messages==TRUE){ cat("\n\nSorting columns and reshaping (one row per data entry)...")}
  colnames(data)[1] <- "ID" # selecting and sorting columns
  data <- data[,c("ID","os","ProtocolId","ScriptName","RunTimestamp","SubmissionTimestamp","InputId","Response")]
  data <- reshape(data,v.names=c("Response"),timevar=c("InputId"), # reshaping from long (item-by-item) to wide (resp-by-resp)
                  idvar=c("RunTimestamp","SubmissionTimestamp"),direction=c("wide"),sep="")
  colnames(data) <- gsub("Response","",colnames(data)) # removing label "Response" from ResponseId
  data <- data[order(data$ID,data$RunTimestamp),] # sorting rows by participant ID and RunTimestamp
  data <- data.frame(cbind(data[1:3],data[ncol(data)],data[5:ncol(data)-1])) # final sorting of rows and columns
  data <- data[order(data$ID,data$RunTimestamp),]
  row.names(data) <- as.character(1:nrow(data))
  if(messages==TRUE){ cat("\n\nRead",nrow(data),"responses from",nlevels(as.factor(data$ID)),"participants.") }
  return(data) } # returning processed dataset
```
</p></details>

<br>

Here, we use the `readSurveyData` function (depicted above) to read and preliminary recode the .JSON data files, and to merge them into the `ESM` dataset. In one single case, the JSON string contains (illegal) UTF8 byte-order-mark.
```{r }
# data reading, encoding and saving
ESM <- readSurveyData(data.path="ESM/data",probe.definition="ESM/probe")
```

<br>

We also integrate the `ESM` dataset with **4 additional responses** that were sent via instant messages (i.e., screenshot of the responses) due to technical problems. These were stored in the `screenshotX4.rda` file.
```{r }
load("ESM/screenshot_surevyMalfunctioning/screenshotX4.RData") # loading additiona responses
cat("Adding",nrow(screenshotX4),"additional responses sent via instant messages") # N = 4
ESM <- rbind(ESM,screenshotX4) # adding additional responses to the ESM dataset
```

<br>

Finally, we import the **"baseline surveys"** data (i.e., ESM data collected in the lab at the beginning of each day, using [Google Forms](https://www.google.com/intl/it/forms/about/)), exported from Google Form as a .CSV file. Some recoding procedures will be necessary before merging the `baseline` and the `ESM` datasets.
```{r }
# reading raw dataset exported from Google Forms
baseline <- read.csv("baseline/Baseline.csv",header=TRUE)
cat("Reading",nrow(baseline),"responses to the baseline questionnaire from",
    nlevels(as.factor(baseline$Nome.utente)),"participants")
```

<br>

ARRIVATO QUI

## 2.2. Data recoding

Then, we recode the `ESM` and `baseline` variables to be used for the analyses, and we merge the two datasets. 

The following packages and functions are used to optimize the ESM data recoding:
```{r warning=FALSE,message=FALSE}
library(lubridate); library(scales)
```

<details><summary>`within.day.adjust`</summary>
<p>
```{r }
within.day.adjust <- function(data){
  
  # Creating within.day.indicator (dafault = 0)
  data$within.day <- NA
  
  # adjusting within.day based on the scheduled times
  for(i in 1:nrow(data)){
    if(strftime(data[i,"RunTimestamp",], # survey 1 between 11:20 (- 10 min error) and 11:40 (up to 12.00 + 10min error)
                format="%H:%M:%S")>strftime("1970-01-01 11:10:00",
                                            format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                          format="%H:%M:%S")<strftime("1970-01-01 12:10:00",
                                                                                                      format="%H:%M:%S")){
      data[i,"within.day"] = 1 } 
    else if(strftime(data[i,"RunTimestamp",], # survey 2 between 12:30 (- 10min error) and 12:50 (up to 13:10 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 12:20:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 13:20:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 2 } 
    else if(strftime(data[i,"RunTimestamp",], # survey 3 between 13:40 (- 10min error) and 14:00 (up to 14:20 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 13:30:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 14:30:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 3} 
    else if(strftime(data[i,"RunTimestamp",], # survey 4 between 14:50 (- 10min error) and 15:10 (up to 15.30 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 14:40:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 15:40:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 4 } 
    else if(strftime(data[i,"RunTimestamp",], # survey 5 between 16:00 (- 10min error) and 16:20 (up to 16:40 + 10min error)
                     format="%H:%M:%S")>strftime("1970-01-01 15:50:00",
                                                 format="%H:%M:%S") & strftime(data[i,"RunTimestamp",],
                                                                               format="%H:%M:%S")<strftime("1970-01-01 16:50:00",
                                                                                                           format="%H:%M:%S")){
      data[i,"within.day"] = 5 }}
  
  # sorting dataset based on ID, day.of.week and within.day
  data <- data[order(data$ID,data$day.of.week,data$within.day),]
  data$within.day <- as.factor(data$within.day)
  rownames(data) <- 1:nrow(data)
  cat("Creating/Adjusting within.day variable...\n Printing summary of cases:\n")
  print(summary(data$within.day))
  return(data) }
```
</p></details>

Creates or adjusts the `within.day` variable based on the scheduled temporal windows used in the current study.

<details><summary>`varsRecoding`</summary>
<p>
```{r }
varsRecoding <- function(data){

  # Recoding item scores and reversed items
  data$v3.positivo.negativo <- 8 - data$v3.positivo.negativo # HEDONIC TONE = positive
  colnames(data)[which(colnames(data)=="v2.soddisfatto.insoddisfatto")] <- "v2.insoddisfatto.soddisfatto" # changing wrong label
  data$t1.rilassato.teso <- 8 - data$t1.rilassato.teso # CALMNESS = positive
  data$e2.pieno.privodenergia <- 8 - data$e2.pieno.privodenergia # ENERGETIC AROUSAL = positive

  # Recoding confounders
  data$alcohol <- as.factor(gsub("No","0",gsub("Sì","1",data$alcohol))) # alcohol (from yes/no --> 1/0)
  data$Activity <- as.factor(gsub("Leggerasedutiocoricatiperlamaggiorpartedeltempo","0", # physical activity level (0 = seated)
                                  gsub("Moderatacamminata,farelescale","0", # (1 = moderate)
                                       gsub("Intensaattivitàsportiva,palestra,corsaodiverserampediscale","2", # (2 = vigorous)
                                            data$Activity))))
  data$People.rec <- as.factor(gsub("No,erodasola","0",gsub("No,erodasolo","0", # (0=alone)
                                gsub("Sì,manonsonostatadisturbata","1",gsub("Sì,manonsonostatodisturbato","1", # (1=not interact)
                                     gsub("Sì,esonostataunpo'disturbata","2",gsub("Sì,esonostataunpo'disturbato","2", # (2=disturb)
                                          gsub("Sì,esonostatainterrotta","3",gsub("Sì,esonostatainterrotto","3", #(3 = interrupted)
                                               data$People.rec)))))))))
  data$ProtocolId <- as.factor(gsub("ProtocolStudent","",data$ProtocolId)) # from protocol ID to sex
  colnames(data)[which(colnames(data)=="ProtocolId")] <- "sex"
  
  # Sorting and renaming columns
  data <- data[,c("ID","os","sex","day.of.week","within.day","RunTimestamp", # response information
                  "v1.male.bene","v2.insoddisfatto.soddisfatto","v3.positivo.negativo", # hedonic tone
                  "t1.rilassato.teso","t2.agitato.calmo","t3.nervoso.tranquillo", # calmness
                  "e1.stanco.sveglio","e2.pieno.privodenergia","e3.affaticato.fresco", # energetic arousal
                  "PE","PE.int","NE","NE.int", # emotional events
                  "smoke","coffe","alcohol","Activity","People.rec")] # confounders
  colnames(data)[7:ncol(data)] <- c("v1","v2","v3","t1","t2","t3","f1","f2","f3", # renaming columns
                                    "PE","intensity.PE","NE","intensity.NE","smoke","coffe","alcohol","activity","people")
  
  
  
  # final sorting and returning data
  data$sort <- as.numeric(substr(data$ID,4,6))
  data <- data[order(data$sort,data$RunTimestamp),] # sorting by ID and timestamp
  data$sort <- NULL # removing sort column
  return(data) }
```
</p></details>

Recodes slider and multiple-choice responses collected with the Sensus protocols used in the study, sort the columns and fix problems due to Daylight Time changes.

<details><summary>`blsRecoding`</summary>
<p>
```{r }
blsRecoding <- function(baseline){ require(textclean); require(mgsub)
  
  # renaming and sorting columns
  colnames(baseline) <- c("RunTimestamp","ID","v1","t1","f1","v2","t2","f2","v3","t3","f3",
                          "PE","PE.int","NE","NE.int","Sleep1","Sleep2","Sleep3","daybefore.ex","drugs","drugs.which")
  baseline <- baseline[,c(2,1,3,6,9,4,7,10,5,8,11:21)]

  # recoding item scores
  baseline$v2 <- 8 - baseline$v2 # hedonic tone = positive
  baseline$v3 <- 8 - baseline$v3
  baseline$t2 <- 8 - baseline$t2 # t.arousal = negative
  baseline$t3 <- 8 - baseline$t3
  baseline$f2 <- 8 - baseline$f2 # e.arousal = positive
  baseline$Sleep2 <- 8 - baseline$Sleep2 # sleep quality = positive
  baseline$Sleep3 <- 8 - baseline$Sleep3

  # other confounding variables
  baseline$drugs <- gsub("SÃ¬","1",baseline$drugs) # drugs
  baseline$drugs <- as.factor(gsub("No","0",baseline$drugs))
  baseline$daybefore.ex <- gsub("SÃ¬","1",baseline$daybefore.ex) # intense exercise on the day before
  baseline$daybefore.ex <- as.factor(gsub("No","0",baseline$daybefore.ex))

  # timestamp variables
  baseline$RunTimestamp <- as.POSIXct(as.character(baseline$RunTimestamp),
                                      format="%Y/%m/%d %H:%M:%S")
  baseline$day.of.week <- as.POSIXlt(baseline$RunTimestamp)$wday # creating day of week
  baseline$within.day <- 0
  baseline <- baseline[,c(1,2,22,23,3:21)]
  baseline <- baseline[order(baseline$ID,baseline$RunTimestamp),] # sorting by timestamp
  
  return(baseline)}
```
</p></details>

<br>

### 2.2.1. ID recoding

As a first step, we **recode participants' identification codes** `ID` (currently corresponding to their e-mail addresses) using the "HRVXXX" format (e.g., from '[john.smith\@gmail.com](mailto:john.smith@gmail.com){.email}' to 'HRV001'). This is done with the `prel.qs_IDrecode()` and the `participantID_recoding_baseline()`  functions. Again, since the participants' e-mail addresses are confidential, both the function and the original dataset are not showed.
```{r }
# loading and using the function to recode the ID variable
source("participantID_recoding.R"); source("participantID_recoding_baseline.R")
(ESM <- participantID_recoding(ESM))[1:3,] # recoding ESM IDs and showing first three rows
colnames(baseline)[2] <- "ID"
(baseline <- participantID_recoding_baseline(baseline))[1:3,] # recoding baseline IDs and showing first three rows
```

<br>

### 2.2.2. Timestamps

Here, we process the **timestamps** (i.e., temporal variables) in both the `EMA` and the `baseline` datasets.

First, we convert the included timestamps **from character to POSIXct** (the R format for dates and times), using the "GMT" time zone (note that data were collected in the CET/CEST region, but GMT is easier to handle).
```{r }
# converting temporal variables as POSIXct in the ESM dataset
ESM[,c("RunTimestamp","SubmissionTimestamp")] <- # no need to specify format since they already are POSIXct
  lapply(ESM[,c("RunTimestamp","SubmissionTimestamp")],function(x) as.POSIXct(x,tz="GMT"))

# converting baseline variables as POSIXct in the baseline dataset
colnames(baseline)[which(colnames(baseline)=="Informazioni.cronologiche")] <- "RunTimestamp"
baseline$RunTimestamp <- as.POSIXct(baseline$RunTimestamp,format="%Y/%m/%d %I:%M:%S %p",tz="GMT")
```

<br>

Second, we inspect the data for cases of **wrongly encoded timestamps**, considering that data collection started on November 6th, 2018 and ended on November 21th, 2019. Only two cases are detected, whose `RunTimestamp` was recorded with the wrong year.
```{r }
# min and max RunTimestamp
c(min(ESM$RunTimestamp),max(ESM$RunTimestamp)) # ESM: unexpected values < November 2018
c(min(baseline$RunTimestamp),max(baseline$RunTimestamp)) # baseline: ok

# only two cases with RunTimestamp < November 2018 and no SubmissionTimestamp
ESM[ESM$RunTimestamp < as.POSIXct("2018-11-06") | ESM$RunTimestamp > as.POSIXct("2019-11-22"),
    c("ID","RunTimestamp","SubmissionTimestamp")]
# plotting all timestamps from these two participants: isolated point recorded as it was collected one year before
grid.arrange(ggplot(ESM[ESM$ID=="HRV046",],aes(RunTimestamp)) + geom_histogram(bins=30) + ggtitle("HRV046"),
             ggplot(ESM[ESM$ID=="HRV048",],aes(RunTimestamp)) + geom_histogram(bins=30) + ggtitle("HRV048"))

# correcting wrongly encoded RunTimestamps (subtracting 1year) and SubmissionTimestamps (RunTimestamp + 4 min)
ESM[substr(ESM$RunTimestamp,1,7)=="2018-01","SubmissionTimestamp"] <-
  ESM[substr(ESM$RunTimestamp,1,7)=="2018-01","RunTimestamp"] + 4*60
ESM[substr(ESM$RunTimestamp,1,7)=="2018-01",c("RunTimestamp","SubmissionTimestamp")] <-
  ESM[substr(ESM$RunTimestamp,1,7)=="2018-01",c("RunTimestamp","SubmissionTimestamp")] + 24*60*60*365
c(min(ESM$RunTimestamp),max(ESM$RunTimestamp)) # sanity check: min and max matching with the data collection period

# 2 further cases with missing SubmissionTimestamp (replacing with RunTimestamp + 4 min)
ESM[is.na(ESM$SubmissionTimestamp),c("ID","RunTimestamp","SubmissionTimestamp")]
ESM[is.na(ESM$SubmissionTimestamp),"SubmissionTimestamp"] <- ESM[is.na(ESM$SubmissionTimestamp),"RunTimestamp"] + 4*60
```

<br>

#### Daylight saving time

Third, we **adjust the temporal coordinates** accounting for **daylight saving time**. Indeed, whereas timestamps were recorded based on the internal clock of participants' mobile phones, which automatically synchronizes based on time changes, such time shifts got lost in the data exporting or the reading, resulting in **one-hour shifts during standard time**. In Italy, during the data collection period, daylight saving time was applied from March 31th to October 27th 2019. Here, we visually inspect the timestamp of the first response for both the `EMA` and the `baseline` datasets.
```{r fig.width=8,fig.height=3}
# ESM: computing and plotting time without date
ESM$daytime <- as.POSIXct(paste(hour(ESM$RunTimestamp),minute(ESM$RunTimestamp)),format="%H %M",tz="GMT")
ESM$daylight <- "standard"
ESM[ESM$RunTimestamp > as.POSIXct("2019-04-01") & ESM$RunTimestamp < as.POSIXct("2019-10-27"),"daylight"] <- "daylight"
ggplot(ESM[!duplicated(ESM$ID),],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("ESM$RunTimestamp of the first response for each participant")

# baseline: computing and plotting time without date
baseline$daytime <- as.POSIXct(paste(hour(baseline$RunTimestamp),minute(baseline$RunTimestamp)),format="%H %M",tz="GMT")
baseline$daylight <- "standard"
baseline[baseline$RunTimestamp>as.POSIXct("2019-04-01") & baseline$RunTimestamp<as.POSIXct("2019-10-27"),"daylight"]<-"daylight"
ggplot(baseline[,],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("baseline$RunTimestamp")
```

<br>

*Comments:*

- in the `EMA` dataset, the responses recorded during **standard time** were encoded with **one hour behind** the actual time (i.e., the first questionnaire was scheduled at 11:30, whereas most responses were recorded at 10:30), while those recorded during **daylight saving time** showing **one additional hour behind** the actual time (i.e., most responses recorded at 9:30)

- in the `baseline` dataset, the responses recorded during **daylight saving time** were recorded with **one hour behind** the actual time (i.e., baseline questionnaires were administered around 10:30, whereas most responses are around 9:30), while those recorded during **standard time** looks fine

<br>

Here, we adjust the temporal coordinates in both datasets according to the observed time shifts.
```{r fig.width=8,fig.height=3}
# adding 1 hour to all ESM timestamps
ESM[,c("RunTimestamp","SubmissionTimestamp")] <- ESM[,c("RunTimestamp","SubmissionTimestamp")] + 1*60*60
# adding 1 further hour to ESM timestamps during daylightsaving time
ESM[ESM$daylight=="daylight",c("RunTimestamp","SubmissionTimestamp")] <- 
  ESM[ESM$daylight=="daylight",c("RunTimestamp","SubmissionTimestamp")] + 1*60*60

# adding 1 hour to baseline timestamps during daylightsaving time
baseline[baseline$daylight=="daylight","RunTimestamp"] <- baseline[baseline$daylight=="daylight","RunTimestamp"] + 1*60*60

# ESM: computing and plotting time without date (NOW OK)
ESM$daytime <- as.POSIXct(paste(hour(ESM$RunTimestamp),minute(ESM$RunTimestamp)),format="%H %M",tz="GMT")
ggplot(ESM[!duplicated(ESM$ID),],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("ESM$RunTimestamp of the first response for each participant")

# baseline: computing and plotting time without date (NOW OK)
baseline$daytime <- as.POSIXct(paste(hour(baseline$RunTimestamp),minute(baseline$RunTimestamp)),format="%H %M",tz="GMT")
ggplot(baseline[,],aes(y=daytime,fill=daylight)) + geom_bar(position=position_dodge(),width=50) +
  scale_y_datetime(labels = date_format("%H:%M"),date_breaks="60 min") +
  ggtitle("baseline$RunTimestamp of the first response for each participant")
```

<br>

#### day.of.week & within.day

Finally, we create two variables to better organize the temporal structure of the datasets: 
- `day.of.week` indicating the weekday indicator (i.e., 1 = Tuesday, 2 = Wednesday, 3 = Thursday)
- `within.day` consisting in a row identifier within each day. For `baseline` responses, we set `within.day = 0`
```{r }
# creating indicator for the weekday 
ESM$day.of.week <- as.POSIXlt(ESM$RunTimestamp)$wday - 1
baseline$day.of.week <- as.POSIXlt(baseline$RunTimestamp)$wday - 1
levels(as.factor(c(levels(as.factor(ESM$day.of.week)),levels(as.factor(baseline$day.of.week))))) # sanity check: ok

# creating row identifier within each day
ESM <- plyr::ddply(ESM,c("ID","day.of.week"),transform,within.day=seq_along(day.of.week))
baseline$within.day <- 0 # assigning zero to the baseline questionnaires

# showing data structure
ESM[,c("ID","day.of.week","within.day","RunTimestamp")]
```

<br>

However, the method used above to encode the `within.day` variable counts the survey as they are received (i.e., first, second, third), and not as they were *scheduled* (i.e., based on the `RunTimestamp` variable), neglecting missing responses. 

Here, we use the `within.day.adjust()` function to recode the `within.day` variable accounting for the following **scheduled temporal windows** and the **expiration time** (i.e., after 20 minutes), and we add **10 extra minutes** before and after the following time limits to account for the variability among devices:

-   *0* = 09:45 \~ 11:00 (*baseline survey*)

-   *1* = 11:20 - 11:40 + 20 min (up to 12:00)

-   *2* = 12:30 - 12:50 + 20 min (up to 13:10)

-   *3* = 13:40 - 14:00 + 20 min (up to 14:20)

-   *4* = 14:50 - 15:10 + 20 min (up to 15:50)

-   *5* = 16:00 - 16:20 + 20 min (up to 16:40)

Here, we apply the function and we correct one case of missing `within.day` (i.e., `RunTimestamp` = 17:40; recorded as `within.day = 5`).
```{r message=FALSE,warning=FALSE}
# adjusting within.day based on scheduled time windows
ESM <- within.day.adjust(ESM)

# correcting 1 missing case (17:40 -> within.day = 5)
ESM[is.na(ESM$within.day),"within.day"] <- 5

# showing data structure
ESM[,c("ID","day.of.week","within.day","RunTimestamp")]
```

<br>

### 2.2.3. Other variables

Qua invece usi le due funzioni recode qui sopra


# 3. HRV data

# 4. GNG data

# 5. Data merging

# 6. Data dictionary

# 7. Data export

<br>

# References {#ref}

- Xiong, H., Huang, Y., Barnes, L. E., & Gerber, M. S. (2016). Sensus: a cross-platform, general-purpose system for mobile crowdsensing in human-subject studies. *Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing*, 415–426. https://doi.org/10.1145/2971648.2971711

## R packages
